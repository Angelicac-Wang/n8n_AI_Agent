#!/usr/bin/env python3
import os
import json
import re

def extract_node_name_from_filename(filename):
    """å¾æª”æ¡ˆåæå–ç¯€é»åç¨±"""
    # ç§»é™¤å‰¯æª”å
    name = filename.replace('.json', '')
    # ç§»é™¤å¯èƒ½çš„å‰ç¶´
    if name.startswith('n8n-nodes-base.'):
        name = name[15:]  # ç§»é™¤ 'n8n-nodes-base.' å‰ç¶´
    return name.lower()

def extract_node_name_from_content(file_path):
    """å¾æª”æ¡ˆå…§å®¹æå–ç¯€é»åç¨±"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å˜—è©¦ä¸åŒçš„æ¬„ä½
        for field in ['name', 'displayName']:
            if field in data:
                name = data[field]
                # æ¸…ç†åç¨±
                if name.startswith('n8n-nodes-base.'):
                    name = name[15:]
                return name.lower()
        
        return None
    except:
        return None

def analyze_duplicates():
    """åˆ†æç¬¬ä¸€æ¬¡å’Œç¬¬ä¸‰æ¬¡ fetch çš„é‡è¤‡æƒ…æ³"""
    
    base_path = "/Users/angelicawang/Documents/n8n/n8n_json_schema"
    first_fetch_path = os.path.join(base_path, "node_schemas")
    third_fetch_path = os.path.join(base_path, "nodes_base_schemas")
    
    # æ”¶é›†ç¬¬ä¸€æ¬¡ fetch çš„ç¯€é»
    first_fetch_nodes = {}
    if os.path.exists(first_fetch_path):
        for filename in os.listdir(first_fetch_path):
            if filename.endswith('.json') and not filename.startswith('.'):
                file_path = os.path.join(first_fetch_path, filename)
                node_name = extract_node_name_from_content(file_path)
                if node_name:
                    first_fetch_nodes[node_name] = {
                        'filename': filename,
                        'source': 'API',
                        'path': file_path
                    }
    
    # æ”¶é›†ç¬¬ä¸‰æ¬¡ fetch çš„ç¯€é»
    third_fetch_nodes = {}
    if os.path.exists(third_fetch_path):
        for filename in os.listdir(third_fetch_path):
            if filename.endswith('.json') and not filename.startswith('.'):
                file_path = os.path.join(third_fetch_path, filename)
                node_name = extract_node_name_from_content(file_path)
                if node_name:
                    third_fetch_nodes[node_name] = {
                        'filename': filename,
                        'source': 'nodes-base package',
                        'path': file_path
                    }
    
    # åˆ†æé‡è¤‡å’Œæ–°å¢
    duplicates = []
    unique_to_first = []
    unique_to_third = []
    
    all_nodes = set(first_fetch_nodes.keys()) | set(third_fetch_nodes.keys())
    
    for node_name in all_nodes:
        in_first = node_name in first_fetch_nodes
        in_third = node_name in third_fetch_nodes
        
        if in_first and in_third:
            duplicates.append({
                'node': node_name,
                'first_file': first_fetch_nodes[node_name]['filename'],
                'third_file': third_fetch_nodes[node_name]['filename']
            })
        elif in_first and not in_third:
            unique_to_first.append({
                'node': node_name,
                'filename': first_fetch_nodes[node_name]['filename']
            })
        elif not in_first and in_third:
            unique_to_third.append({
                'node': node_name,
                'filename': third_fetch_nodes[node_name]['filename']
            })
    
    # å ±å‘Šçµæœ
    print(f"ğŸ“Š é‡è¤‡åˆ†æçµæœ:")
    print(f"ç¬¬ä¸€æ¬¡ fetch (API): {len(first_fetch_nodes)} å€‹ç¯€é»")
    print(f"ç¬¬ä¸‰æ¬¡ fetch (nodes-base): {len(third_fetch_nodes)} å€‹ç¯€é»")
    print(f"é‡è¤‡çš„ç¯€é»: {len(duplicates)} å€‹")
    print(f"åƒ…åœ¨ç¬¬ä¸€æ¬¡ fetch: {len(unique_to_first)} å€‹")
    print(f"åƒ…åœ¨ç¬¬ä¸‰æ¬¡ fetch: {len(unique_to_third)} å€‹")
    
    print(f"\\nğŸ”„ é‡è¤‡ç¯€é»ç¯„ä¾‹ (å‰10å€‹):")
    for dup in duplicates[:10]:
        print(f"  {dup['node']}: {dup['first_file']} <-> {dup['third_file']}")
    
    print(f"\\nğŸ†• åƒ…åœ¨ç¬¬ä¸‰æ¬¡ fetch çš„æ–°ç¯€é» (å‰20å€‹):")
    for unique in unique_to_third[:20]:
        print(f"  {unique['node']}: {unique['filename']}")
    
    print(f"\\nğŸ“‹ åƒ…åœ¨ç¬¬ä¸€æ¬¡ fetch çš„ç¯€é» (å‰20å€‹):")
    for unique in unique_to_first[:20]:
        print(f"  {unique['node']}: {unique['filename']}")
    
    # è©³ç´°åˆ†æç¬¬ä¸‰æ¬¡ fetch çš„æ–°å…§å®¹
    print(f"\\nğŸ” ç¬¬ä¸‰æ¬¡ fetch æ–°å¢å…§å®¹åˆ†æ:")
    
    # æŒ‰é¡åˆ¥åˆ†ææ–°ç¯€é»
    categories = {}
    for unique in unique_to_third:
        node_name = unique['node']
        # ç°¡å–®çš„åˆ†é¡é‚è¼¯
        if 'trigger' in node_name:
            category = 'Triggers'
        elif any(word in node_name for word in ['google', 'microsoft', 'aws', 'azure']):
            category = 'Cloud Services'
        elif any(word in node_name for word in ['database', 'mysql', 'postgres', 'mongo']):
            category = 'Databases'
        elif any(word in node_name for word in ['email', 'mail', 'smtp']):
            category = 'Email'
        else:
            category = 'Others'
        
        if category not in categories:
            categories[category] = []
        categories[category].append(node_name)
    
    print(f"\\nğŸ“‚ æ–°ç¯€é»åˆ†é¡:")
    for category, nodes in categories.items():
        print(f"  {category}: {len(nodes)} å€‹ç¯€é»")
        for node in nodes[:5]:  # é¡¯ç¤ºå‰5å€‹
            print(f"    - {node}")
        if len(nodes) > 5:
            print(f"    ... é‚„æœ‰ {len(nodes) - 5} å€‹")
    
    # é‡è¤‡ç‡è¨ˆç®—
    if len(third_fetch_nodes) > 0:
        overlap_rate = len(duplicates) / len(third_fetch_nodes) * 100
        new_content_rate = len(unique_to_third) / len(third_fetch_nodes) * 100
        print(f"\\nğŸ“ˆ çµ±è¨ˆ:")
        print(f"é‡è¤‡ç‡: {overlap_rate:.1f}%")
        print(f"æ–°å…§å®¹ç‡: {new_content_rate:.1f}%")
    
    return {
        'duplicates': duplicates,
        'unique_to_first': unique_to_first,
        'unique_to_third': unique_to_third,
        'first_count': len(first_fetch_nodes),
        'third_count': len(third_fetch_nodes)
    }

if __name__ == "__main__":
    results = analyze_duplicates()