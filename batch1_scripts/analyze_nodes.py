#!/usr/bin/env python3
"""
åˆ†æ n8n ç¯€é»é¡å‹å’Œç¤¾ç¾¤ç¯€é»çµ±è¨ˆ
"""

import os
import json
from collections import defaultdict

def analyze_node_types():
    """åˆ†æç¯€é»é¡å‹å’Œä¾†æº"""
    schema_dir = "/Users/angelicawang/Documents/n8n/n8n_json_schema/node_schemas"
    
    if not os.path.exists(schema_dir):
        print("âŒ node_schemas ç›®éŒ„ä¸å­˜åœ¨")
        return
    
    # çµ±è¨ˆè®Šæ•¸
    total_nodes = 0
    official_nodes = 0
    langchain_nodes = 0
    ai_related_nodes = 0
    tool_nodes = 0
    trigger_nodes = 0
    
    # åˆ†é¡çµ±è¨ˆ
    categories = defaultdict(int)
    node_types = defaultdict(list)
    
    # AI ç›¸é—œé—œéµå­—
    ai_keywords = ['openai', 'anthropic', 'gemini', 'gpt', 'ai', 'llm', 'langchain', 
                   'embeddings', 'sentiment', 'classifier', 'chatbot', 'assistant']
    
    print("=== n8n ç¯€é»åˆ†æå ±å‘Š ===")
    print(f"åˆ†æç›®éŒ„: {schema_dir}")
    print()
    
    # éæ­·æ‰€æœ‰ç¯€é»æª”æ¡ˆ
    for filename in os.listdir(schema_dir):
        if not filename.endswith('.json'):
            continue
            
        total_nodes += 1
        filepath = os.path.join(schema_dir, filename)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                node_data = json.load(f)
            
            node_name = node_data.get('name', '')
            display_name = node_data.get('displayName', '')
            description = node_data.get('description', '')
            
            # åˆ†é¡ç¯€é»ä¾†æº
            if 'langchain' in node_name.lower() or 'langchain' in filename.lower():
                langchain_nodes += 1
                categories['LangChain'] += 1
                node_types['LangChain'].append(display_name)
            elif node_name.startswith('n8n-nodes-base.'):
                official_nodes += 1
                categories['Official'] += 1
                node_types['Official'].append(display_name)
            else:
                categories['Community'] += 1
                node_types['Community'].append(display_name)
            
            # æª¢æŸ¥æ˜¯å¦ç‚º AI ç›¸é—œç¯€é»
            text_to_check = f"{node_name} {display_name} {description}".lower()
            if any(keyword in text_to_check for keyword in ai_keywords):
                ai_related_nodes += 1
                categories['AI-Related'] += 1
            
            # æª¢æŸ¥ç¯€é»é¡å‹
            if 'tool' in display_name.lower() or 'tool' in filename.lower():
                tool_nodes += 1
                categories['Tools'] += 1
            
            if 'trigger' in display_name.lower() or 'trigger' in filename.lower():
                trigger_nodes += 1
                categories['Triggers'] += 1
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å– {filename}: {e}")
    
    # è¼¸å‡ºçµ±è¨ˆçµæœ
    print("ğŸ“Š ç¯€é»ç¸½é‡çµ±è¨ˆ:")
    print(f"  ç¸½ç¯€é»æ•¸: {total_nodes}")
    print(f"  å®˜æ–¹ç¯€é»: {official_nodes} ({official_nodes/total_nodes*100:.1f}%)")
    print(f"  LangChain ç¯€é»: {langchain_nodes} ({langchain_nodes/total_nodes*100:.1f}%)")
    print(f"  ç¤¾ç¾¤ç¯€é»: {categories['Community']} ({categories['Community']/total_nodes*100:.1f}%)")
    print()
    
    print("ğŸ¤– AI ç›¸é—œçµ±è¨ˆ:")
    print(f"  AI ç›¸é—œç¯€é»: {ai_related_nodes} ({ai_related_nodes/total_nodes*100:.1f}%)")
    print()
    
    print("âš™ï¸ åŠŸèƒ½é¡å‹çµ±è¨ˆ:")
    print(f"  å·¥å…·ç¯€é»: {tool_nodes}")
    print(f"  è§¸ç™¼å™¨ç¯€é»: {trigger_nodes}")
    print()
    
    # é¡¯ç¤º LangChain ç¯€é»è©³æƒ…
    if langchain_nodes > 0:
        print("ğŸ”— LangChain ç¯€é»è©³ç´°åˆ—è¡¨:")
        langchain_files = [f for f in os.listdir(schema_dir) if f.endswith('.json') and ('langchain' in f.lower() or 'anthropic' in f.lower() or 'openai' in f.lower())]
        
        langchain_categories = {
            'LLM Models': [],
            'Chat Models': [],
            'Embeddings': [],
            'Vector Stores': [],
            'Memory': [],
            'Tools': [],
            'Chains': [],
            'Agents': [],
            'Other': []
        }
        
        for filename in sorted(langchain_files):
            filepath = os.path.join(schema_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    node_data = json.load(f)
                display_name = node_data.get('displayName', filename.replace('.json', ''))
                
                # åˆ†é¡ LangChain ç¯€é»
                name_lower = display_name.lower()
                if 'chat model' in name_lower:
                    langchain_categories['Chat Models'].append(display_name)
                elif 'model' in name_lower and 'chat' not in name_lower:
                    langchain_categories['LLM Models'].append(display_name)
                elif 'embedding' in name_lower:
                    langchain_categories['Embeddings'].append(display_name)
                elif 'vector' in name_lower:
                    langchain_categories['Vector Stores'].append(display_name)
                elif 'memory' in name_lower:
                    langchain_categories['Memory'].append(display_name)
                elif 'tool' in name_lower:
                    langchain_categories['Tools'].append(display_name)
                elif 'chain' in name_lower:
                    langchain_categories['Chains'].append(display_name)
                elif 'agent' in name_lower:
                    langchain_categories['Agents'].append(display_name)
                else:
                    langchain_categories['Other'].append(display_name)
                    
            except Exception as e:
                print(f"  âš ï¸ ç„¡æ³•è®€å– {filename}: {e}")
        
        for category, nodes in langchain_categories.items():
            if nodes:
                print(f"  {category} ({len(nodes)} å€‹):")
                for node in sorted(nodes)[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    print(f"    - {node}")
                if len(nodes) > 5:
                    print(f"    ... é‚„æœ‰ {len(nodes) - 5} å€‹")
                print()
    
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print(f"ä½ çš„ n8n ä¼ºæœå™¨ç¾åœ¨æ”¯æ´ {total_nodes} å€‹ç¯€é»ï¼ŒåŒ…å«å¤§é‡ AI/LangChain åŠŸèƒ½ï¼")

if __name__ == "__main__":
    analyze_node_types()