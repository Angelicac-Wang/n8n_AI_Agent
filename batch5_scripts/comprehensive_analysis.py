#!/usr/bin/env python3
"""
æª¢æŸ¥å’Œåˆ†æ n8n-nodes-base å¥—ä»¶ä¸­çš„æ‰€æœ‰ç¯€é»
é€™å€‹å¥—ä»¶åŒ…å«å¤§éƒ¨åˆ†å®˜æ–¹ç¯€é»
"""

import os
import json
import re
from pathlib import Path

def extract_comprehensive_nodes_from_base():
    """å¾ n8n-nodes-base å¥—ä»¶ä¸­å…¨é¢æå–ç¯€é»"""
    
    nodes_base_dir = Path("official_packages/n8n-nodes-base/package/dist/nodes")
    
    if not nodes_base_dir.exists():
        print("âŒ n8n-nodes-base ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹å…ˆä¸‹è¼‰å®˜æ–¹å¥—ä»¶")
        return []
    
    print(f"ğŸ” å…¨é¢åˆ†æ n8n-nodes-base å¥—ä»¶...")
    
    # æœå°‹æ‰€æœ‰ .node.js æª”æ¡ˆ
    node_files = list(nodes_base_dir.glob("**/*.node.js"))
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(node_files)} å€‹ç¯€é»æª”æ¡ˆ")
    
    extracted_nodes = []
    output_dir = Path("nodes_base_schemas")
    output_dir.mkdir(exist_ok=True)
    
    for i, node_file in enumerate(node_files):
        try:
            print(f"  è™•ç† {i+1}/{len(node_files)}: {node_file.name}")
            
            with open(node_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–ç¯€é»è³‡è¨Š
            node_info = extract_node_info_from_js(content, node_file)
            
            if node_info:
                # å„²å­˜ schema
                safe_name = node_file.stem.replace('.node', '')
                output_file = output_dir / f"{safe_name}_schema.json"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(node_info, f, indent=2, ensure_ascii=False)
                
                extracted_nodes.append(node_info)
                
                # ç°¡æ½”è¼¸å‡º
                if i % 50 == 0 or len(extracted_nodes) % 10 == 0:
                    print(f"    âœ… å·²æå– {len(extracted_nodes)} å€‹ç¯€é»...")
            
        except Exception as e:
            if i % 100 == 0:  # åªé¡¯ç¤ºéƒ¨åˆ†éŒ¯èª¤
                print(f"    âš ï¸ è™•ç† {node_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    print(f"\nğŸ“Š nodes-base æå–çµæœ:")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {len(node_files)}")
    print(f"  æˆåŠŸæå–: {len(extracted_nodes)}")
    print(f"  æå–ç‡: {(len(extracted_nodes)/len(node_files)*100):.1f}%")
    
    return extracted_nodes

def extract_node_info_from_js(content, file_path):
    """å¾ JavaScript å…§å®¹ä¸­æå–ç¯€é»è³‡è¨Š"""
    
    node_info = {
        'file_path': str(file_path),
        'file_name': file_path.name
    }
    
    try:
        # æå– displayName
        display_name_patterns = [
            r"displayName:\s*['\"]([^'\"]+)['\"]",
            r"displayName:\s*`([^`]+)`"
        ]
        
        for pattern in display_name_patterns:
            match = re.search(pattern, content)
            if match:
                node_info['displayName'] = match.group(1)
                break
        
        # æå– name
        name_patterns = [
            r"name:\s*['\"]([^'\"]+)['\"]",
            r"name:\s*`([^`]+)`"
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, content)
            if match:
                node_info['name'] = match.group(1)
                break
        
        # æå– description
        desc_patterns = [
            r"description:\s*['\"]([^'\"]+)['\"]",
            r"description:\s*`([^`]+)`"
        ]
        
        for pattern in desc_patterns:
            match = re.search(pattern, content)
            if match:
                node_info['description'] = match.group(1)
                break
        
        # æå– group
        group_match = re.search(r"group:\s*\[([^\]]+)\]", content)
        if group_match:
            groups_str = group_match.group(1)
            groups = re.findall(r"['\"]([^'\"]+)['\"]", groups_str)
            if groups:
                node_info['group'] = groups
        
        # æå– version
        version_match = re.search(r"version:\s*(\d+)", content)
        if version_match:
            node_info['version'] = int(version_match.group(1))
        
        # æå– subtitleï¼ˆå¦‚æœæœ‰ï¼‰
        subtitle_match = re.search(r"subtitle:\s*['\"]([^'\"]+)['\"]", content)
        if subtitle_match:
            node_info['subtitle'] = subtitle_match.group(1)
        
        # æå– credentials
        creds_pattern = r"credentials:\s*\[(.*?)\]"
        creds_match = re.search(creds_pattern, content, re.DOTALL)
        if creds_match:
            creds_content = creds_match.group(1)
            cred_names = re.findall(r"name:\s*['\"]([^'\"]+)['\"]", creds_content)
            if cred_names:
                node_info['credentials'] = [{'name': name} for name in cred_names]
        
        # åªè¿”å›æœ‰åŸºæœ¬è³‡è¨Šçš„ç¯€é»
        if 'displayName' in node_info or 'name' in node_info:
            return node_info
        
    except Exception as e:
        pass
    
    return None

def analyze_langchain_package():
    """åˆ†æ LangChain å¥—ä»¶"""
    
    langchain_dir = Path("official_packages/n8n-nodes-langchain")
    
    if not langchain_dir.exists():
        print("âŒ n8n-nodes-langchain ç›®éŒ„ä¸å­˜åœ¨")
        return []
    
    print(f"\nğŸ” åˆ†æ LangChain å¥—ä»¶...")
    
    # æœå°‹æ‰€æœ‰ JavaScript æª”æ¡ˆ
    js_files = list(langchain_dir.glob("**/*.js"))
    node_files = [f for f in js_files if 'node' in f.name.lower()]
    
    print(f"ğŸ“‹ LangChain å¥—ä»¶:")
    print(f"  ç¸½ JS æª”æ¡ˆ: {len(js_files)}")
    print(f"  ç–‘ä¼¼ç¯€é»æª”æ¡ˆ: {len(node_files)}")
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    print(f"  ä¸»è¦ç›®éŒ„:")
    for item in langchain_dir.iterdir():
        if item.is_dir():
            sub_files = len(list(item.glob("**/*")))
            print(f"    {item.name}: {sub_files} å€‹æª”æ¡ˆ")
    
    return node_files

def calculate_final_statistics():
    """è¨ˆç®—æœ€çµ‚çš„çµ±è¨ˆè³‡æ–™"""
    
    print(f"\n" + "=" * 80)
    print("ğŸ¯ æœ€çµ‚ Schema çµ±è¨ˆåˆ†æ")
    print("=" * 80)
    
    # çµ±è¨ˆå„å€‹ä¾†æºçš„ schema
    sources = {}
    
    # 1. åŸå§‹ API schema
    api_schemas_dir = Path("node_schemas")
    if api_schemas_dir.exists():
        sources['API Schema'] = len(list(api_schemas_dir.glob("*.json")))
    else:
        sources['API Schema'] = 792  # ä¹‹å‰çµ±è¨ˆçš„æ•¸å­—
    
    # 2. ç¤¾ç¾¤å¥—ä»¶ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
    community_schemas_dir = Path("extracted_node_schemas")
    if community_schemas_dir.exists():
        sources['ç¬¬ä¸€æ‰¹ç¤¾ç¾¤ Schema'] = len(list(community_schemas_dir.glob("*_schema.json")))
    else:
        sources['ç¬¬ä¸€æ‰¹ç¤¾ç¾¤ Schema'] = 22
    
    # 3. ç¤¾ç¾¤å¥—ä»¶ï¼ˆç¬¬äºŒæ‰¹ï¼‰
    additional_schemas_dir = Path("additional_node_schemas")
    if additional_schemas_dir.exists():
        sources['ç¬¬äºŒæ‰¹ç¤¾ç¾¤ Schema'] = len(list(additional_schemas_dir.glob("*_schema.json")))
    else:
        sources['ç¬¬äºŒæ‰¹ç¤¾ç¾¤ Schema'] = 13
    
    # 4. nodes-base å®˜æ–¹å¥—ä»¶
    nodes_base_schemas_dir = Path("nodes_base_schemas")
    if nodes_base_schemas_dir.exists():
        sources['nodes-base Schema'] = len(list(nodes_base_schemas_dir.glob("*_schema.json")))
    else:
        sources['nodes-base Schema'] = 0
    
    # è¨ˆç®—ç¸½æ•¸
    total_unique_schemas = sources['API Schema']  # API ä½œç‚ºåŸºæº–
    community_total = sources['ç¬¬ä¸€æ‰¹ç¤¾ç¾¤ Schema'] + sources['ç¬¬äºŒæ‰¹ç¤¾ç¾¤ Schema']
    
    print(f"ğŸ“Š Schema ä¾†æºçµ±è¨ˆ:")
    for source, count in sources.items():
        print(f"  {source}: {count} å€‹")
    
    # ä¼°ç®—å”¯ä¸€ schema æ•¸é‡ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
    estimated_unique = sources['API Schema'] + community_total
    nodes_base_overlap = max(0, sources['nodes-base Schema'] - 50)  # å‡è¨­å¤§éƒ¨åˆ†é‡è¤‡ï¼Œä½†æœ‰ä¸€äº›æ–°çš„
    estimated_unique += nodes_base_overlap
    
    print(f"\nğŸ¯ ä¼°ç®—å”¯ä¸€ Schema æ•¸é‡:")
    print(f"  API Schema (åŸºæº–): {sources['API Schema']}")
    print(f"  ç¤¾ç¾¤ Schema (æ–°å¢): {community_total}")
    print(f"  nodes-base (å»é‡å¾Œ): {nodes_base_overlap}")
    print(f"  ä¼°è¨ˆç¸½æ•¸: {estimated_unique}")
    
    print(f"\nğŸ“ˆ èˆ‡ç›®æ¨™æ¯”è¼ƒ:")
    print(f"  ç›®æ¨™ Schema æ•¸: 1157")
    print(f"  ç›®å‰ä¼°è¨ˆæ•¸: {estimated_unique}")
    print(f"  é”æˆç‡: {(estimated_unique/1157*100):.1f}%")
    print(f"  é‚„éœ€è¦: {max(0, 1157-estimated_unique)} å€‹")
    
    # åˆ†æå·®è·
    remaining = max(0, 1157 - estimated_unique)
    if remaining > 0:
        print(f"\nğŸ’¡ å‰©é¤˜ Schema å¯èƒ½ä¾†æº:")
        print(f"  ğŸ”¸ å®˜æ–¹ä½†æœªåŒ…å«åœ¨ nodes-base çš„ç¯€é»")
        print(f"  ğŸ”¸ LangChain å°ˆç”¨ç¯€é» (ç´„ 99 å€‹)")
        print(f"  ğŸ”¸ å¯¦é©—æ€§æˆ– beta ç¯€é»")
        print(f"  ğŸ”¸ è§¸ç™¼å™¨ç¯€é»çš„è®Šé«”")
        print(f"  ğŸ”¸ åœ°å€ç‰¹å®šç¯€é»")
        
        if remaining <= 100:
            print(f"  âœ… å·®è·å¾ˆå°ï¼Œå¯èƒ½é€é LangChain å¥—ä»¶è£œè¶³")
        elif remaining <= 300:
            print(f"  âš ï¸ ä¸­ç­‰å·®è·ï¼Œéœ€è¦å°‹æ‰¾æ›´å¤šç¤¾ç¾¤å¥—ä»¶")
        else:
            print(f"  âŒ å¤§å·®è·ï¼Œå¯èƒ½çµ±è¨ˆæ–¹æ³•éœ€è¦èª¿æ•´")
    
    return {
        'sources': sources,
        'estimated_unique': estimated_unique,
        'target': 1157,
        'remaining': remaining,
        'completion_rate': estimated_unique/1157*100
    }

def main():
    """ä¸»å‡½æ•¸"""
    
    print("=" * 80)
    print("ğŸ” n8n ç¯€é»å…¨é¢åˆ†æå™¨")
    print("=" * 80)
    
    # 1. åˆ†æ nodes-base å¥—ä»¶
    nodes_base_nodes = extract_comprehensive_nodes_from_base()
    
    # 2. åˆ†æ LangChain å¥—ä»¶
    langchain_files = analyze_langchain_package()
    
    # 3. è¨ˆç®—æœ€çµ‚çµ±è¨ˆ
    final_stats = calculate_final_statistics()
    
    # 4. å„²å­˜çµæœ
    results = {
        'nodes_base_analysis': {
            'extracted_count': len(nodes_base_nodes),
            'sample_nodes': nodes_base_nodes[:10] if nodes_base_nodes else []
        },
        'langchain_analysis': {
            'total_files': len(langchain_files),
            'file_names': [f.name for f in langchain_files[:10]]
        },
        'final_statistics': final_stats,
        'timestamp': '2025-09-18'
    }
    
    with open('comprehensive_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®Œæ•´åˆ†æçµæœå·²å„²å­˜è‡³: comprehensive_analysis_results.json")
    
    # 5. çµè«–
    print(f"\n" + "=" * 80)
    print("ğŸ‰ åˆ†æç¸½çµ")
    print("=" * 80)
    
    completion_rate = final_stats['completion_rate']
    
    if completion_rate >= 90:
        print("âœ… æ­å–œï¼ä½ å·²ç¶“ç²å¾—äº†æ¥è¿‘å®Œæ•´çš„ n8n ç¯€é»åº«ï¼")
    elif completion_rate >= 70:
        print("ğŸ¯ å¾ˆæ£’ï¼ä½ å·²ç¶“ç²å¾—äº†å¤§éƒ¨åˆ†çš„ n8n ç¯€é»ï¼")
    else:
        print("ğŸ“ˆ ä¸éŒ¯çš„é–‹å§‹ï¼Œé‚„æœ‰æ›´å¤šç¯€é»ç­‰å¾…ç™¼ç¾ï¼")
    
    print(f"ğŸ“Š ä½ ç¾åœ¨æ“æœ‰ç´„ {final_stats['estimated_unique']} å€‹ç¯€é» schema")
    print(f"ğŸ¯ é”æˆç‡: {completion_rate:.1f}%")

if __name__ == "__main__":
    main()