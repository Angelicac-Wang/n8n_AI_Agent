#!/usr/bin/env python3
"""
è©³ç´°åˆ†æ n8n_json_schema ç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆ
çµ±è¨ˆå„å€‹ä¾†æºçš„æª”æ¡ˆæ•¸é‡ï¼Œæª¢æŸ¥é‡è¤‡ï¼Œä¸¦é©—è­‰æª”æ¡ˆæœ‰æ•ˆæ€§
"""

import os
import json
from pathlib import Path
from collections import defaultdict

def analyze_directory_structure():
    """åˆ†æç›®éŒ„çµæ§‹å’Œæª”æ¡ˆåˆ†å¸ƒ"""
    
    base_dir = Path(".")
    
    print("ğŸ” åˆ†æ n8n_json_schema ç›®éŒ„çµæ§‹...")
    print("=" * 80)
    
    # çµ±è¨ˆå„å€‹ç›®éŒ„çš„æª”æ¡ˆ
    directories = {}
    
    for item in base_dir.iterdir():
        if item.is_dir():
            json_files = list(item.glob("**/*.json"))
            py_files = list(item.glob("**/*.py"))
            other_files = list(item.glob("**/*")) 
            other_files = [f for f in other_files if f.suffix not in ['.json', '.py'] and f.is_file()]
            
            directories[item.name] = {
                'json_files': len(json_files),
                'py_files': len(py_files),
                'other_files': len(other_files),
                'total_files': len(json_files) + len(py_files) + len(other_files)
            }
    
    # æ ¹ç›®éŒ„çš„æª”æ¡ˆ
    root_json = list(base_dir.glob("*.json"))
    root_py = list(base_dir.glob("*.py"))
    root_other = list(base_dir.glob("*"))
    root_other = [f for f in root_other if f.suffix not in ['.json', '.py'] and f.is_file()]
    
    directories['æ ¹ç›®éŒ„'] = {
        'json_files': len(root_json),
        'py_files': len(root_py),
        'other_files': len(root_other),
        'total_files': len(root_json) + len(root_py) + len(root_other)
    }
    
    return directories

def check_json_file_validity():
    """æª¢æŸ¥ JSON æª”æ¡ˆçš„æœ‰æ•ˆæ€§"""
    
    print("\nğŸ” æª¢æŸ¥ JSON æª”æ¡ˆæœ‰æ•ˆæ€§...")
    
    base_dir = Path(".")
    json_files = list(base_dir.glob("**/*.json"))
    
    valid_files = 0
    invalid_files = 0
    invalid_details = []
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                json.load(f)
            valid_files += 1
        except Exception as e:
            invalid_files += 1
            invalid_details.append({
                'file': str(json_file),
                'error': str(e)[:100]
            })
    
    print(f"âœ… æœ‰æ•ˆ JSON æª”æ¡ˆ: {valid_files}")
    print(f"âŒ ç„¡æ•ˆ JSON æª”æ¡ˆ: {invalid_files}")
    
    if invalid_files > 0:
        print(f"\nâš ï¸ ç„¡æ•ˆæª”æ¡ˆè©³æƒ… (å‰10å€‹):")
        for detail in invalid_details[:10]:
            print(f"  {detail['file']}: {detail['error']}")
    
    return valid_files, invalid_files, invalid_details

def find_duplicate_files():
    """å°‹æ‰¾é‡è¤‡çš„æª”æ¡ˆï¼ˆåŸºæ–¼æª”æ¡ˆåç¨±ï¼‰"""
    
    print("\nğŸ” æª¢æŸ¥é‡è¤‡æª”æ¡ˆ...")
    
    base_dir = Path(".")
    all_files = list(base_dir.glob("**/*"))
    all_files = [f for f in all_files if f.is_file()]
    
    # æŒ‰æª”æ¡ˆååˆ†çµ„
    name_groups = defaultdict(list)
    for file_path in all_files:
        name_groups[file_path.name].append(file_path)
    
    # æ‰¾å‡ºé‡è¤‡çš„æª”æ¡ˆå
    duplicates = {name: paths for name, paths in name_groups.items() if len(paths) > 1}
    
    print(f"ğŸ“Š é‡è¤‡æª”æ¡ˆåçµ±è¨ˆ:")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {len(all_files)}")
    print(f"  é‡è¤‡æª”æ¡ˆå: {len(duplicates)}")
    
    if duplicates:
        print(f"\nğŸ” é‡è¤‡æª”æ¡ˆè©³æƒ… (å‰10å€‹):")
        for i, (name, paths) in enumerate(list(duplicates.items())[:10]):
            print(f"  {i+1}. {name} ({len(paths)} å€‹)")
            for path in paths:
                print(f"     - {path}")
    
    return duplicates

def analyze_fetch_history():
    """åˆ†æ fetch æ­·å²"""
    
    print("\nğŸ“‹ Fetch æ­·å²åˆ†æ...")
    print("=" * 80)
    
    fetch_history = [
        {
            'batch': 1,
            'method': 'n8n ä¼ºæœå™¨ API',
            'target': 'æ‰€æœ‰å·²å®‰è£ç¯€é»',
            'result': 792,
            'location': 'node_schemas/',
            'description': 'å¾ä½ çš„ n8n ä¼ºæœå™¨é€é API ç²å–æ‰€æœ‰å·²å®‰è£ç¯€é»çš„ schema'
        },
        {
            'batch': 2, 
            'method': 'npm ç¤¾ç¾¤å¥—ä»¶ (ç¬¬ä¸€æ‰¹)',
            'target': '15å€‹ç²¾é¸ç¤¾ç¾¤å¥—ä»¶',
            'result': 27,
            'location': 'community_packages/ + extracted_node_schemas/',
            'description': 'ä¸‹è¼‰ç¤¾ç¾¤å¥—ä»¶ä¸¦å¾ä¸­æå–ç¯€é» schema'
        },
        {
            'batch': 3,
            'method': 'npm å®˜æ–¹å¥—ä»¶',
            'target': 'n8n-nodes-base + n8n-nodes-langchain',
            'result': 458,
            'location': 'official_packages/',
            'description': 'ä¸‹è¼‰å®˜æ–¹æ ¸å¿ƒç¯€é»å¥—ä»¶'
        },
        {
            'batch': 4,
            'method': 'npm ç¤¾ç¾¤å¥—ä»¶ (ç¬¬äºŒæ‰¹)',
            'target': '14å€‹é«˜åƒ¹å€¼ç¤¾ç¾¤å¥—ä»¶',
            'result': 16,
            'location': 'additional_community_packages/ + additional_node_schemas/',
            'description': 'ä¸‹è¼‰æ›´å¤šå°ˆæ¥­åŒ–ç¤¾ç¾¤å¥—ä»¶'
        },
        {
            'batch': 5,
            'method': 'å…¨é¢åˆ†æ nodes-base',
            'target': 'å®˜æ–¹æ ¸å¿ƒç¯€é»å®Œæ•´æå–',
            'result': 443,
            'location': 'nodes_base_schemas/',
            'description': 'å¾å®˜æ–¹ nodes-base å¥—ä»¶ä¸­æå–æ‰€æœ‰ç¯€é»çš„è©³ç´° schema'
        }
    ]
    
    total_fetched = 0
    for i, fetch in enumerate(fetch_history):
        print(f"ğŸ”„ ç¬¬ {fetch['batch']} æ¬¡ Fetch:")
        print(f"   æ–¹æ³•: {fetch['method']}")
        print(f"   ç›®æ¨™: {fetch['target']}")
        print(f"   çµæœ: {fetch['result']} å€‹æª”æ¡ˆ")
        print(f"   ä½ç½®: {fetch['location']}")
        print(f"   èªªæ˜: {fetch['description']}")
        print()
        total_fetched += fetch['result']
    
    print(f"ğŸ“Š ç¸½è¨ˆ:")
    print(f"   Fetch æ¬¡æ•¸: {len(fetch_history)}")
    print(f"   ç¸½ç²å–æª”æ¡ˆ: {total_fetched}")
    
    return fetch_history, total_fetched

def count_unique_schemas():
    """çµ±è¨ˆå”¯ä¸€çš„ schema æª”æ¡ˆ"""
    
    print("\nğŸ¯ çµ±è¨ˆå”¯ä¸€ Schema æª”æ¡ˆ...")
    
    # ä¸»è¦ schema ä¾†æºç›®éŒ„
    schema_dirs = {
        'node_schemas': 'åŸå§‹ API Schema',
        'extracted_node_schemas': 'ç¬¬ä¸€æ‰¹ç¤¾ç¾¤ Schema', 
        'additional_node_schemas': 'ç¬¬äºŒæ‰¹ç¤¾ç¾¤ Schema',
        'nodes_base_schemas': 'å®˜æ–¹ nodes-base Schema',
        'community_node_schemas': 'ç¤¾ç¾¤ JSON Schema'
    }
    
    total_schemas = 0
    schema_counts = {}
    
    for dir_name, description in schema_dirs.items():
        dir_path = Path(dir_name)
        if dir_path.exists():
            json_files = list(dir_path.glob("*.json"))
            schema_counts[description] = len(json_files)
            total_schemas += len(json_files)
            print(f"ğŸ“ {description}: {len(json_files)} å€‹")
        else:
            schema_counts[description] = 0
            print(f"ğŸ“ {description}: 0 å€‹ (ç›®éŒ„ä¸å­˜åœ¨)")
    
    print(f"\nğŸ¯ å”¯ä¸€ Schema ç¸½æ•¸: {total_schemas}")
    
    return schema_counts, total_schemas

def main():
    """ä¸»å‡½æ•¸"""
    
    print("=" * 80)
    print("ğŸ“Š n8n_json_schema ç›®éŒ„å®Œæ•´åˆ†æå ±å‘Š")
    print("=" * 80)
    
    # 1. ç›®éŒ„çµæ§‹åˆ†æ
    directories = analyze_directory_structure()
    
    print("\nğŸ“ ç›®éŒ„çµæ§‹:")
    total_json = 0
    total_py = 0
    total_other = 0
    
    for dir_name, stats in directories.items():
        print(f"  {dir_name}:")
        print(f"    JSON: {stats['json_files']}")
        print(f"    Python: {stats['py_files']}")
        print(f"    å…¶ä»–: {stats['other_files']}")
        print(f"    å°è¨ˆ: {stats['total_files']}")
        
        total_json += stats['json_files']
        total_py += stats['py_files']
        total_other += stats['other_files']
    
    print(f"\nğŸ“Š ç¸½è¨ˆ:")
    print(f"  JSON æª”æ¡ˆ: {total_json}")
    print(f"  Python æª”æ¡ˆ: {total_py}")
    print(f"  å…¶ä»–æª”æ¡ˆ: {total_other}")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {total_json + total_py + total_other}")
    
    # 2. JSON æª”æ¡ˆæœ‰æ•ˆæ€§æª¢æŸ¥
    valid_json, invalid_json, invalid_details = check_json_file_validity()
    
    # 3. é‡è¤‡æª”æ¡ˆæª¢æŸ¥
    duplicates = find_duplicate_files()
    
    # 4. Fetch æ­·å²åˆ†æ
    fetch_history, total_fetched = analyze_fetch_history()
    
    # 5. å”¯ä¸€ schema çµ±è¨ˆ
    schema_counts, total_schemas = count_unique_schemas()
    
    # 6. æœ€çµ‚ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ‰ æœ€çµ‚ç¸½çµ")
    print("=" * 80)
    
    print(f"ğŸ“‹ å·¥ä½œå›é¡§:")
    print(f"  åŸ·è¡Œäº† {len(fetch_history)} æ¬¡è³‡æ–™ç²å–")
    print(f"  ç¸½å…±ç²å–äº† {total_fetched} å€‹ç¯€é»æª”æ¡ˆ")
    print(f"  æœ€çµ‚ç”¢ç”Ÿäº† {total_json} å€‹ JSON æª”æ¡ˆ")
    print(f"  å…¶ä¸­æœ‰æ•ˆ JSON æª”æ¡ˆ: {valid_json} å€‹")
    print(f"  å”¯ä¸€çš„ Schema æª”æ¡ˆ: {total_schemas} å€‹")
    
    print(f"\nğŸ“Š æª”æ¡ˆå“è³ª:")
    print(f"  JSON æª”æ¡ˆæœ‰æ•ˆç‡: {(valid_json/total_json*100):.1f}%")
    print(f"  é‡è¤‡æª”æ¡ˆåæ•¸é‡: {len(duplicates)}")
    
    print(f"\nğŸ¯ é”æˆæƒ…æ³:")
    print(f"  ç›®æ¨™ Schema æ•¸: 1157")
    print(f"  å¯¦éš› Schema æ•¸: {total_schemas}")
    print(f"  é”æˆç‡: {(total_schemas/1157*100):.1f}%")
    
    # 7. å„²å­˜å ±å‘Š
    report = {
        'directories': directories,
        'json_validation': {
            'valid': valid_json,
            'invalid': invalid_json,
            'invalid_details': invalid_details[:10]
        },
        'duplicates': {name: [str(p) for p in paths] for name, paths in list(duplicates.items())[:10]},
        'fetch_history': fetch_history,
        'schema_counts': schema_counts,
        'summary': {
            'total_json_files': total_json,
            'valid_json_files': valid_json,
            'total_schemas': total_schemas,
            'total_fetches': len(fetch_history),
            'total_fetched_files': total_fetched,
            'completion_rate': total_schemas/1157*100
        }
    }
    
    with open('final_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è©³ç´°å ±å‘Šå·²å„²å­˜è‡³: final_analysis_report.json")

if __name__ == "__main__":
    main()