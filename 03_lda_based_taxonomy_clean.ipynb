{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA-Based Taxonomy Tree for n8n Node Schemas\n",
    "\n",
    "ÈÄôÂÄã notebook ÈáùÂ∞ç n8n node schemas ÈÄ≤Ë°å LDA ÂàÜÊûêÔºö\n",
    "1. Ëß£Êûê node_schemas/ ÁõÆÈåÑ‰∏≠ÁöÑ JSON Ê™îÊ°à\n",
    "2. ÂÖàÈÄèÈÅé LDA ÂÅµÊ∏¨Âá∫‰∏çÂêåÁöÑ topics\n",
    "3. Ëº∏Âá∫ topic, words, Â∞çÊáâÁöÑ node schema Ê™î\n",
    "4. Âà©Áî®ÈÄô‰∫õ topics Âª∫Á´ã taxonomy tree\n",
    "5. Â∞àÊ≥®Êñº n8n nodes ÁöÑÂäüËÉΩÂàÜÈ°û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent\n",
      "Working directory: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent\n",
      "Node schemas path: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/node_schemas\n",
      "Found 792 JSON schema files\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ë®≠ÂÆöÂ∞àÊ°àË∑ØÂæë\n",
    "project_root = Path().absolute()\n",
    "node_schemas_path = project_root / \"node_schemas\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {Path().absolute()}\")\n",
    "print(f\"Node schemas path: {node_schemas_path}\")\n",
    "\n",
    "# Ê™¢Êü• node_schemas ÁõÆÈåÑÊòØÂê¶Â≠òÂú®\n",
    "if not node_schemas_path.exists():\n",
    "    print(f\"Error: {node_schemas_path} does not exist!\")\n",
    "else:\n",
    "    schema_files = list(node_schemas_path.glob(\"*.json\"))\n",
    "    print(f\"Found {len(schema_files)} JSON schema files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeSchemaParser class created successfully!\n"
     ]
    }
   ],
   "source": [
    "class NodeSchemaParser:\n",
    "    def __init__(self, schemas_dir):\n",
    "        self.schemas_dir = Path(schemas_dir)\n",
    "    \n",
    "    def extract_text_from_json(self, data, exclude_keys=None):\n",
    "        if exclude_keys is None:\n",
    "            exclude_keys = {'type', 'required', 'default', 'noDataExpression'}\n",
    "        \n",
    "        texts = []\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in exclude_keys:\n",
    "                    continue\n",
    "                if isinstance(value, str) and len(value.strip()) > 1:\n",
    "                    texts.append(value.strip())\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    texts.extend(self.extract_text_from_json(value, exclude_keys))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                texts.extend(self.extract_text_from_json(item, exclude_keys))\n",
    "        elif isinstance(data, str) and len(data.strip()) > 1:\n",
    "            texts.append(data.strip())\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def parse_schema_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                schema_data = json.load(f)\n",
    "            \n",
    "            name = schema_data.get('name', file_path.stem)\n",
    "            display_name = schema_data.get('displayName', '')\n",
    "            description = schema_data.get('description', '')\n",
    "            title = display_name if display_name else name\n",
    "            \n",
    "            all_texts = self.extract_text_from_json(schema_data)\n",
    "            content = ' '.join(all_texts)\n",
    "            \n",
    "            return {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'name': name,\n",
    "                'display_name': display_name,\n",
    "                'description': description,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'raw_data': schema_data\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_all_schemas(self):\n",
    "        schemas = {}\n",
    "        schema_files = list(self.schemas_dir.glob(\"*.json\"))\n",
    "        \n",
    "        for file_path in schema_files:\n",
    "            schema_info = self.parse_schema_file(file_path)\n",
    "            if schema_info:\n",
    "                schemas[schema_info['file_name']] = schema_info\n",
    "        \n",
    "        return schemas\n",
    "\n",
    "print(\"NodeSchemaParser class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicModeler class created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        self.n_topics = n_topics\n",
    "        self.random_state = random_state\n",
    "        self.vectorizer = None\n",
    "        self.lda_model = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        processed_docs = [self.preprocess_text(doc) for doc in documents]\n",
    "        processed_docs = [doc for doc in processed_docs if len(doc.strip()) > 0]\n",
    "        \n",
    "        self.vectorizer = CountVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        doc_term_matrix = self.vectorizer.fit_transform(processed_docs)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=self.n_topics,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=10,\n",
    "            learning_method='online'\n",
    "        )\n",
    "        \n",
    "        doc_topic_probs = self.lda_model.fit_transform(doc_term_matrix)\n",
    "        perplexity = self.lda_model.perplexity(doc_term_matrix)\n",
    "        \n",
    "        results = self._generate_results(doc_topic_probs)\n",
    "        results['model_perplexity'] = perplexity\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_results(self, doc_topic_probs):\n",
    "        results = {\n",
    "            'topics': [],\n",
    "            'document_topics': [],\n",
    "            'topic_similarities': []\n",
    "        }\n",
    "        \n",
    "        for topic_idx in range(self.n_topics):\n",
    "            topic_words = []\n",
    "            topic_weights = self.lda_model.components_[topic_idx]\n",
    "            top_indices = topic_weights.argsort()[-10:][::-1]\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                topic_words.append({\n",
    "                    'word': self.feature_names[idx],\n",
    "                    'weight': float(topic_weights[idx])\n",
    "                })\n",
    "            \n",
    "            top_words = [w['word'] for w in topic_words[:3]]\n",
    "            label = ' + '.join(top_words).title()\n",
    "            coherence = np.mean([w['weight'] for w in topic_words[:5]])\n",
    "            \n",
    "            results['topics'].append({\n",
    "                'id': topic_idx,\n",
    "                'label': label,\n",
    "                'top_words': topic_words,\n",
    "                'coherence': float(coherence)\n",
    "            })\n",
    "        \n",
    "        for doc_idx, topic_probs in enumerate(doc_topic_probs):\n",
    "            dominant_topic = int(np.argmax(topic_probs))\n",
    "            dominant_prob = float(np.max(topic_probs))\n",
    "            \n",
    "            results['document_topics'].append({\n",
    "                'document_index': doc_idx,\n",
    "                'dominant_topic': dominant_topic,\n",
    "                'dominant_topic_probability': dominant_prob,\n",
    "                'all_topic_probabilities': topic_probs.tolist()\n",
    "            })\n",
    "        \n",
    "        topic_similarities = np.corrcoef(self.lda_model.components_)\n",
    "        results['topic_similarities'] = topic_similarities.tolist()\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"TopicModeler class created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: ËºâÂÖ•ÂíåËß£Êûê Node Schema Ê™îÊ°à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 792 schema files\n",
      "First few files: ['onfleet.json', 'lemlistTool.json', 'chat.json', 'wait.json', 'crowdDevTool.json']\n",
      "\n",
      "Example schema: onfleet.json\n",
      "Display name: Onfleet\n",
      "Description: Consume Onfleet API...\n",
      "Content length: 22165 characters\n",
      "Title: Onfleet\n"
     ]
    }
   ],
   "source": [
    "# Ëß£Êûê Node Schema Ê™îÊ°à\n",
    "parser = NodeSchemaParser(node_schemas_path)\n",
    "schemas = parser.parse_all_schemas()\n",
    "\n",
    "print(f\"Successfully parsed {len(schemas)} schema files\")\n",
    "print(f\"First few files: {list(schemas.keys())[:5]}\")\n",
    "\n",
    "# È°ØÁ§∫ÁØÑ‰æã schema ÂÖßÂÆπ\n",
    "if schemas:\n",
    "    first_schema = next(iter(schemas.values()))\n",
    "    print(f\"\\nExample schema: {first_schema['file_name']}\")\n",
    "    print(f\"Display name: {first_schema['display_name']}\")\n",
    "    print(f\"Description: {first_schema['description'][:100]}...\")\n",
    "    print(f\"Content length: {len(first_schema['content'])} characters\")\n",
    "    print(f\"Title: {first_schema['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Ê∫ñÂÇôÊñá‰ª∂ÂÖßÂÆπÈÄ≤Ë°å LDA ÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 792 documents for LDA analysis\n",
      "Content length - Min: 131, Max: 136209, Mean: 6732\n"
     ]
    }
   ],
   "source": [
    "# Ê∫ñÂÇôÊñá‰ª∂ÂÖßÂÆπ\n",
    "documents = []\n",
    "file_mapping = []\n",
    "\n",
    "for file_path, schema_data in schemas.items():\n",
    "    content = schema_data[\"content\"]\n",
    "    if len(content.strip()) > 50:\n",
    "        documents.append(content)\n",
    "        file_mapping.append({\n",
    "            \"file_path\": schema_data[\"file_path\"],\n",
    "            \"file_name\": schema_data[\"file_name\"],\n",
    "            \"title\": schema_data[\"title\"],\n",
    "            \"display_name\": schema_data[\"display_name\"],\n",
    "            \"description\": schema_data[\"description\"]\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(documents)} documents for LDA analysis\")\n",
    "\n",
    "content_lengths = [len(doc) for doc in documents]\n",
    "print(f\"Content length - Min: {min(content_lengths)}, Max: {max(content_lengths)}, Mean: {np.mean(content_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Âü∑Ë°å LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA analysis completed!\n",
      "Generated 10 topics\n",
      "Model perplexity: 264.39\n"
     ]
    }
   ],
   "source": [
    "# Âü∑Ë°å LDA\n",
    "topic_modeler = TopicModeler()\n",
    "lda_results = topic_modeler.fit_transform(documents)\n",
    "\n",
    "print(f\"LDA analysis completed!\")\n",
    "print(f\"Generated {len(lda_results['topics'])} topics\")\n",
    "print(f\"Model perplexity: {lda_results.get('model_perplexity', 'N/A'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: È°ØÁ§∫ Topics ÂíåÂ∞çÊáâÁöÑÊ™îÊ°à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Topic 0: Contact + Id + Field ===\n",
      "Coherence Score: 1137.054\n",
      "\n",
      "Top Words:\n",
      "  contact              1751.1399\n",
      "  id                   1062.6179\n",
      "  field                1012.8356\n",
      "  dollar               1007.0941\n",
      "  email                851.5806\n",
      "  company              807.1024\n",
      "  custom               779.6133\n",
      "  fields               731.5947\n",
      "  address              652.3607\n",
      "  update               643.7257\n",
      "\n",
      "Files in this topic (38 files with prob >= 0.2):\n",
      "  agileCrm.json                  (0.988)\n",
      "  clearbit.json                  (0.965)\n",
      "  zohoCrm.json                   (0.906)\n",
      "  zohoCrmTool.json               (0.905)\n",
      "  activeCampaign.json            (0.876)\n",
      "  agileCrmTool.json              (0.867)\n",
      "  drift.json                     (0.859)\n",
      "  uplead.json                    (0.857)\n",
      "  activeCampaignTool.json        (0.840)\n",
      "  mailcheck.json                 (0.726)\n",
      "  sendInBlueTrigger.json         (0.714)\n",
      "  googleContacts.json            (0.656)\n",
      "  sort.json                      (0.638)\n",
      "  driftTool.json                 (0.605)\n",
      "  Brandfetch.json                (0.600)\n",
      "  ... and 23 more files\n",
      "\n",
      "=== Topic 1: Id + User + Create ===\n",
      "Coherence Score: 2618.446\n",
      "\n",
      "Top Words:\n",
      "  id                   3657.4041\n",
      "  user                 2829.0132\n",
      "  create               2743.3792\n",
      "  update               2212.3568\n",
      "  delete               1650.0759\n",
      "  list                 1624.8572\n",
      "  getall               1566.1134\n",
      "  return               1394.2086\n",
      "  fields               1242.5840\n",
      "  limit                1218.1836\n",
      "\n",
      "Files in this topic (113 files with prob >= 0.2):\n",
      "  discourse.json                 (0.998)\n",
      "  strava.json                    (0.950)\n",
      "  trelloTool.json                (0.945)\n",
      "  trello.json                    (0.941)\n",
      "  discourseTool.json             (0.939)\n",
      "  sendy.json                     (0.910)\n",
      "  reddit.json                    (0.894)\n",
      "  iterable.json                  (0.890)\n",
      "  flow.json                      (0.874)\n",
      "  philipsHue.json                (0.871)\n",
      "  flowTrigger.json               (0.855)\n",
      "  netlify.json                   (0.849)\n",
      "  mailerLiteTrigger.json         (0.846)\n",
      "  bitwarden.json                 (0.837)\n",
      "  crowdDev.json                  (0.822)\n",
      "  ... and 98 more files\n",
      "\n",
      "=== Topic 2: America + Utc + Asia ===\n",
      "Coherence Score: 2249.232\n",
      "\n",
      "Top Words:\n",
      "  america              3309.1713\n",
      "  utc                  2852.3001\n",
      "  asia                 1943.6474\n",
      "  00                   1873.2573\n",
      "  europe               1267.7855\n",
      "  africa               1069.1582\n",
      "  pacific              895.3252\n",
      "  contact              818.6865\n",
      "  id                   685.0650\n",
      "  company              564.5723\n",
      "\n",
      "Files in this topic (6 files with prob >= 0.2):\n",
      "  googleCalendar.json            (0.989)\n",
      "  googleCalendarTool.json        (0.957)\n",
      "  freshworksCrm.json             (0.763)\n",
      "  freshworksCrmTool.json         (0.761)\n",
      "  hubspotTool.json               (0.593)\n",
      "  microsoftOutlookTool.json      (0.465)\n",
      "\n",
      "=== Topic 3: Field + Id + 9A ===\n",
      "Coherence Score: 1667.517\n",
      "\n",
      "Top Words:\n",
      "  field                2021.1398\n",
      "  id                   1983.5762\n",
      "  9a                   1628.3841\n",
      "  case                 1491.7951\n",
      "  channel              1212.6901\n",
      "  9a 9a                1157.1845\n",
      "  https                1147.3616\n",
      "  list                 1088.1520\n",
      "  text                 1057.6159\n",
      "  date                 1020.5020\n",
      "\n",
      "Files in this topic (9 files with prob >= 0.2):\n",
      "  theHiveProject.json            (0.754)\n",
      "  theHiveProjectTool.json        (0.737)\n",
      "  notion.json                    (0.640)\n",
      "  notionTool.json                (0.635)\n",
      "  stickyNote.json                (0.631)\n",
      "  microsoftTeams.json            (0.572)\n",
      "  microsoftTeamsTrigger.json     (0.538)\n",
      "  microsoftTeamsTool.json        (0.537)\n",
      "  slackTool.json                 (0.352)\n",
      "\n",
      "=== Topic 4: File + Message + Send ===\n",
      "Coherence Score: 1448.925\n",
      "\n",
      "Top Words:\n",
      "  file                 2451.6695\n",
      "  message              1871.6677\n",
      "  send                 1046.1509\n",
      "  options              944.0800\n",
      "  field                931.0583\n",
      "  text                 849.4014\n",
      "  binary               808.4089\n",
      "  url                  728.5953\n",
      "  data                 709.6837\n",
      "  folder               655.0368\n",
      "\n",
      "Files in this topic (194 files with prob >= 0.2):\n",
      "  googleGeminiTool.json          (0.999)\n",
      "  googleGemini.json              (0.999)\n",
      "  formTrigger.json               (0.997)\n",
      "  ftp.json                       (0.995)\n",
      "  messageBird.json               (0.993)\n",
      "  ssh.json                       (0.993)\n",
      "  sms77.json                     (0.993)\n",
      "  compression.json               (0.992)\n",
      "  anthropic.json                 (0.991)\n",
      "  mocean.json                    (0.986)\n",
      "  yourls.json                    (0.981)\n",
      "  msg91.json                     (0.980)\n",
      "  readWriteFile.json             (0.977)\n",
      "  discord.json                   (0.975)\n",
      "  mqtt.json                      (0.974)\n",
      "  ... and 179 more files\n",
      "\n",
      "=== Topic 5: Id + List + Expressions ===\n",
      "Coherence Score: 4561.897\n",
      "\n",
      "Top Words:\n",
      "  id                   7481.1514\n",
      "  list                 4475.4359\n",
      "  expressions          3645.7049\n",
      "  using                3605.7389\n",
      "  specify              3601.4551\n",
      "  expression           3567.6406\n",
      "  using href           3550.1030\n",
      "  choose list          3538.9472\n",
      "  docs n8n             3535.8091\n",
      "  io code              3527.5139\n",
      "\n",
      "Files in this topic (111 files with prob >= 0.2):\n",
      "  wekanTool.json                 (1.000)\n",
      "  wekan.json                     (1.000)\n",
      "  linear.json                    (0.998)\n",
      "  zendeskTrigger.json            (0.998)\n",
      "  venafiTlsProtectCloudTrigger.json (0.965)\n",
      "  taiga.json                     (0.904)\n",
      "  taigaTool.json                 (0.893)\n",
      "  workableTrigger.json           (0.872)\n",
      "  payPalTrigger.json             (0.833)\n",
      "  linearTool.json                (0.819)\n",
      "  erpNext.json                   (0.815)\n",
      "  webflow.json                   (0.800)\n",
      "  freshservice.json              (0.798)\n",
      "  bitbucketTrigger.json          (0.794)\n",
      "  taigaTrigger.json              (0.792)\n",
      "  ... and 96 more files\n",
      "\n",
      "=== Topic 6: Create + Id + Address ===\n",
      "Coherence Score: 833.277\n",
      "\n",
      "Top Words:\n",
      "  create               1122.4637\n",
      "  id                   1053.5173\n",
      "  address              728.0308\n",
      "  update               637.1029\n",
      "  customer             625.2684\n",
      "  number               557.2910\n",
      "  date                 541.1826\n",
      "  getall               541.1414\n",
      "  key                  530.7236\n",
      "  task                 512.3508\n",
      "\n",
      "Files in this topic (30 files with prob >= 0.2):\n",
      "  stripeTool.json                (0.971)\n",
      "  stripe.json                    (0.963)\n",
      "  wooCommerceTool.json           (0.888)\n",
      "  onfleetTool.json               (0.886)\n",
      "  onfleet.json                   (0.879)\n",
      "  wooCommerce.json               (0.878)\n",
      "  quickbooks.json                (0.858)\n",
      "  quickbooksTool.json            (0.857)\n",
      "  onfleetTrigger.json            (0.746)\n",
      "  unleashedSoftware.json         (0.687)\n",
      "  shopify.json                   (0.679)\n",
      "  shopifyTool.json               (0.666)\n",
      "  marketstack.json               (0.657)\n",
      "  invoiceNinjaTool.json          (0.637)\n",
      "  invoiceNinja.json              (0.632)\n",
      "  ... and 15 more files\n",
      "\n",
      "=== Topic 7: Database + Text + Data ===\n",
      "Coherence Score: 701.121\n",
      "\n",
      "Top Words:\n",
      "  database             996.8126\n",
      "  text                 830.2088\n",
      "  data                 707.1998\n",
      "  value                519.2690\n",
      "  condition            452.1150\n",
      "  notion               445.2124\n",
      "  id                   428.9631\n",
      "  row                  418.2138\n",
      "  workflow             408.1341\n",
      "  table                387.5364\n",
      "\n",
      "Files in this topic (117 files with prob >= 0.2):\n",
      "  merge.json                     (0.995)\n",
      "  scheduleTrigger.json           (0.991)\n",
      "  cron.json                      (0.990)\n",
      "  togglTrigger.json              (0.989)\n",
      "  rssFeedReadTrigger.json        (0.989)\n",
      "  documentJsonInputLoader.json   (0.982)\n",
      "  vectorStoreZepLoad.json        (0.977)\n",
      "  vectorStorePineconeLoad.json   (0.976)\n",
      "  vectorStoreInMemoryLoad.json   (0.959)\n",
      "  toolCalculator.json            (0.957)\n",
      "  toolWikipedia.json             (0.955)\n",
      "  toolWolframAlpha.json          (0.953)\n",
      "  function.json                  (0.950)\n",
      "  functionItem.json              (0.947)\n",
      "  start.json                     (0.944)\n",
      "  ... and 102 more files\n",
      "\n",
      "=== Topic 8: _Blank + Target _Blank + App ===\n",
      "Coherence Score: 783.761\n",
      "\n",
      "Top Words:\n",
      "  _blank               788.8929\n",
      "  target _blank        788.3912\n",
      "  app                  787.8194\n",
      "  target               785.4565\n",
      "  https                768.2469\n",
      "  href                 754.1308\n",
      "  href https           753.1832\n",
      "  tools                752.7316\n",
      "  https app            733.9565\n",
      "  info                 725.6598\n",
      "\n",
      "Files in this topic (2 files with prob >= 0.2):\n",
      "  uprocTool.json                 (1.000)\n",
      "  uproc.json                     (1.000)\n",
      "\n",
      "=== Topic 9: Operation + Set + Id ===\n",
      "Coherence Score: 761.116\n",
      "\n",
      "Top Words:\n",
      "  operation            953.9817\n",
      "  set                  751.1729\n",
      "  id                   725.8734\n",
      "  update               693.9250\n",
      "  value                680.6261\n",
      "  return               662.3623\n",
      "  create               635.2127\n",
      "  list                 633.9112\n",
      "  description          622.4938\n",
      "  api                  597.4068\n",
      "\n",
      "Files in this topic (172 files with prob >= 0.2):\n",
      "  npmTool.json                   (0.996)\n",
      "  npm.json                       (0.994)\n",
      "  stackbyTool.json               (0.994)\n",
      "  snowflakeTool.json             (0.994)\n",
      "  rundeckTool.json               (0.993)\n",
      "  googleBusinessProfileTool.json (0.989)\n",
      "  googleBusinessProfile.json     (0.987)\n",
      "  elasticsearchTool.json         (0.985)\n",
      "  rundeck.json                   (0.985)\n",
      "  metabaseTool.json              (0.982)\n",
      "  executeCommandTool.json        (0.975)\n",
      "  azureCosmosDb.json             (0.974)\n",
      "  mongoDbTool.json               (0.968)\n",
      "  crateDbTool.json               (0.967)\n",
      "  metabase.json                  (0.964)\n",
      "  ... and 157 more files\n"
     ]
    }
   ],
   "source": [
    "def display_topic_details(topic_id, min_probability=0.2):\n",
    "    topic_info = lda_results['topics'][topic_id]\n",
    "    \n",
    "    print(f\"\\n=== Topic {topic_id}: {topic_info['label']} ===\")\n",
    "    print(f\"Coherence Score: {topic_info['coherence']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop Words:\")\n",
    "    for word_info in topic_info['top_words'][:10]:\n",
    "        print(f\"  {word_info['word']: <20} {word_info['weight']:.4f}\")\n",
    "    \n",
    "    topic_files = []\n",
    "    for doc_idx, doc_topic in enumerate(lda_results['document_topics']):\n",
    "        if doc_topic['dominant_topic'] == topic_id and doc_topic['dominant_topic_probability'] >= min_probability:\n",
    "            file_info = file_mapping[doc_idx]\n",
    "            topic_files.append({\n",
    "                **file_info,\n",
    "                'probability': doc_topic['dominant_topic_probability']\n",
    "            })\n",
    "    \n",
    "    topic_files.sort(key=lambda x: x['probability'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nFiles in this topic ({len(topic_files)} files with prob >= {min_probability}):\")\n",
    "    for file_info in topic_files[:15]:\n",
    "        print(f\"  {file_info['file_name']: <30} ({file_info['probability']:.3f})\")\n",
    "    \n",
    "    if len(topic_files) > 15:\n",
    "        print(f\"  ... and {len(topic_files) - 15} more files\")\n",
    "    \n",
    "    return topic_files\n",
    "\n",
    "# È°ØÁ§∫ÊâÄÊúâ topics\n",
    "all_topic_files = {}\n",
    "for i in range(len(lda_results['topics'])):\n",
    "    topic_files = display_topic_details(i)\n",
    "    all_topic_files[i] = topic_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Âª∫Á´ã LDA-Based Taxonomy Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA-Based Taxonomy Tree created successfully!\n",
      "Classification Statistics:\n",
      "  total_files: 792\n",
      "  classified_files: 792\n",
      "  unclassified_files: 0\n",
      "  classification_rate: 100.00\n",
      "  topics_count: 10\n",
      "  avg_files_per_topic: 79.20\n"
     ]
    }
   ],
   "source": [
    "class LDABasedTaxonomyBuilder:\n",
    "    def __init__(self, lda_results, file_mapping, min_probability=0.15):\n",
    "        self.lda_results = lda_results\n",
    "        self.file_mapping = file_mapping\n",
    "        self.min_probability = min_probability\n",
    "        self.taxonomy_tree = {}\n",
    "    \n",
    "    def build_taxonomy(self):\n",
    "        self.taxonomy_tree = {\n",
    "            \"metadata\": {\n",
    "                \"method\": \"LDA_Based_Taxonomy_n8n_Nodes\",\n",
    "                \"data_source\": \"n8n_node_schemas\",\n",
    "                \"n_topics\": len(self.lda_results['topics']),\n",
    "                \"min_probability\": self.min_probability,\n",
    "                \"total_files\": len(self.file_mapping),\n",
    "                \"model_perplexity\": self.lda_results.get('model_perplexity', 'N/A')\n",
    "            },\n",
    "            \"topics\": {},\n",
    "            \"unclassified\": {\n",
    "                \"files\": [],\n",
    "                \"description\": \"Files that don't meet the minimum probability threshold\"\n",
    "            },\n",
    "            \"statistics\": {}\n",
    "        }\n",
    "        \n",
    "        classified_files = set()\n",
    "        \n",
    "        for topic_id, topic_info in enumerate(self.lda_results['topics']):\n",
    "            topic_key = f\"topic_{topic_id}\"\n",
    "            topic_files = []\n",
    "            \n",
    "            for doc_idx, doc_topic in enumerate(self.lda_results['document_topics']):\n",
    "                if (doc_topic['dominant_topic'] == topic_id and \n",
    "                    doc_topic['dominant_topic_probability'] >= self.min_probability):\n",
    "                    \n",
    "                    file_info = self.file_mapping[doc_idx]\n",
    "                    topic_files.append({\n",
    "                        \"file_path\": file_info['file_path'],\n",
    "                        \"file_name\": file_info['file_name'],\n",
    "                        \"display_name\": file_info['display_name'],\n",
    "                        \"description\": file_info['description'],\n",
    "                        \"title\": file_info['title'],\n",
    "                        \"probability\": doc_topic['dominant_topic_probability'],\n",
    "                        \"all_topic_probs\": doc_topic['all_topic_probabilities']\n",
    "                    })\n",
    "                    \n",
    "                    classified_files.add(doc_idx)\n",
    "            \n",
    "            topic_files.sort(key=lambda x: x['probability'], reverse=True)\n",
    "            \n",
    "            self.taxonomy_tree[\"topics\"][topic_key] = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"label\": topic_info['label'],\n",
    "                \"description\": f\"n8n nodes strongly associated with {topic_info['label']}\",\n",
    "                \"files\": topic_files,\n",
    "                \"top_words\": [w['word'] for w in topic_info['top_words'][:10]],\n",
    "                \"coherence\": topic_info['coherence'],\n",
    "                \"file_count\": len(topic_files),\n",
    "                \"avg_probability\": np.mean([f['probability'] for f in topic_files]) if topic_files else 0\n",
    "            }\n",
    "        \n",
    "        unclassified_files = []\n",
    "        for doc_idx, file_info in enumerate(self.file_mapping):\n",
    "            if doc_idx not in classified_files:\n",
    "                doc_topic = self.lda_results['document_topics'][doc_idx]\n",
    "                unclassified_files.append({\n",
    "                    \"file_path\": file_info['file_path'],\n",
    "                    \"file_name\": file_info['file_name'],\n",
    "                    \"display_name\": file_info['display_name'],\n",
    "                    \"description\": file_info['description'],\n",
    "                    \"title\": file_info['title'],\n",
    "                    \"max_probability\": max(doc_topic['all_topic_probabilities']),\n",
    "                    \"dominant_topic\": doc_topic['dominant_topic'],\n",
    "                    \"reason\": \"Below minimum probability threshold\"\n",
    "                })\n",
    "        \n",
    "        self.taxonomy_tree[\"unclassified\"][\"files\"] = unclassified_files\n",
    "        self._generate_statistics()\n",
    "        \n",
    "        return self.taxonomy_tree\n",
    "    \n",
    "    def _generate_statistics(self):\n",
    "        total_files = len(self.file_mapping)\n",
    "        classified_files = sum(topic['file_count'] for topic in self.taxonomy_tree['topics'].values())\n",
    "        unclassified_files = len(self.taxonomy_tree['unclassified']['files'])\n",
    "        \n",
    "        self.taxonomy_tree['statistics'] = {\n",
    "            \"total_files\": total_files,\n",
    "            \"classified_files\": classified_files,\n",
    "            \"unclassified_files\": unclassified_files,\n",
    "            \"classification_rate\": (classified_files / total_files) * 100,\n",
    "            \"topics_count\": len(self.taxonomy_tree['topics']),\n",
    "            \"avg_files_per_topic\": classified_files / len(self.taxonomy_tree['topics']) if self.taxonomy_tree['topics'] else 0\n",
    "        }\n",
    "\n",
    "# Âª∫Á´ã LDA-based taxonomy\n",
    "taxonomy_builder = LDABasedTaxonomyBuilder(lda_results, file_mapping, min_probability=0.15)\n",
    "lda_taxonomy = taxonomy_builder.build_taxonomy()\n",
    "\n",
    "print(\"LDA-Based Taxonomy Tree created successfully!\")\n",
    "print(f\"Classification Statistics:\")\n",
    "for key, value in lda_taxonomy['statistics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: ÂåØÂá∫ÁµêÊûú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ n8n node taxonomy tree exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_taxonomy_tree.json\n",
      "üìÑ Simplified taxonomy summary exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_taxonomy_summary.json\n",
      "üìä Topic-file mapping exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_topic_mapping.csv\n",
      "\n",
      "‚úÖ n8n Node Schema LDA Analysis Complete!\n",
      "üìä Generated 10 topic-based categories\n",
      "üìÅ Classified 792/792 node schemas (100.0%)\n",
      "üìà Average node schemas per topic: 79.2\n"
     ]
    }
   ],
   "source": [
    "# ÂåØÂá∫ÁµêÊûú\n",
    "output_dir = project_root / \"data\" / \"output\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_json_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (bool, int, float, str)) or obj is None:\n",
    "        return obj\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "final_taxonomy = make_json_serializable(lda_taxonomy)\n",
    "\n",
    "# ÂåØÂá∫ÂÆåÊï¥ÁöÑ taxonomy tree\n",
    "taxonomy_output_path = output_dir / \"n8n_node_taxonomy_tree.json\"\n",
    "with open(taxonomy_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_taxonomy, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ n8n node taxonomy tree exported to: {taxonomy_output_path}\")\n",
    "\n",
    "# ÂåØÂá∫Á∞°ÂåñÁâàÊú¨\n",
    "simplified_taxonomy = {\n",
    "    \"summary\": final_taxonomy['statistics'],\n",
    "    \"metadata\": final_taxonomy['metadata'],\n",
    "    \"topics\": {\n",
    "        topic_key: {\n",
    "            \"label\": topic_data['label'],\n",
    "            \"file_count\": topic_data['file_count'],\n",
    "            \"top_words\": topic_data['top_words'][:5],\n",
    "            \"top_files\": [f['file_name'] for f in topic_data['files'][:5]]\n",
    "        }\n",
    "        for topic_key, topic_data in final_taxonomy['topics'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "simplified_output_path = output_dir / \"n8n_node_taxonomy_summary.json\"\n",
    "with open(simplified_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_taxonomy, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"üìÑ Simplified taxonomy summary exported to: {simplified_output_path}\")\n",
    "\n",
    "# ÂåØÂá∫ CSV Â∞çÊáâË°®\n",
    "topic_file_mapping = []\n",
    "for topic_key, topic_data in final_taxonomy['topics'].items():\n",
    "    for file_info in topic_data['files']:\n",
    "        topic_file_mapping.append({\n",
    "            'topic_id': topic_data['topic_id'],\n",
    "            'topic_label': topic_data['label'],\n",
    "            'file_name': file_info['file_name'],\n",
    "            'display_name': file_info['display_name'],\n",
    "            'probability': file_info['probability']\n",
    "        })\n",
    "\n",
    "df_mapping = pd.DataFrame(topic_file_mapping)\n",
    "csv_output_path = output_dir / \"n8n_node_topic_mapping.csv\"\n",
    "df_mapping.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "print(f\"üìä Topic-file mapping exported to: {csv_output_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ n8n Node Schema LDA Analysis Complete!\")\n",
    "print(f\"üìä Generated {final_taxonomy['statistics']['topics_count']} topic-based categories\")\n",
    "print(f\"üìÅ Classified {final_taxonomy['statistics']['classified_files']}/{final_taxonomy['statistics']['total_files']} node schemas ({final_taxonomy['statistics']['classification_rate']:.1f}%)\")\n",
    "print(f\"üìà Average node schemas per topic: {final_taxonomy['statistics']['avg_files_per_topic']:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
