{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA-Based Taxonomy Tree for n8n Node Schemas\n",
    "\n",
    "這個 notebook 針對 n8n node schemas 進行 LDA 分析：\n",
    "1. 解析 node_schemas/ 目錄中的 JSON 檔案\n",
    "2. 先透過 LDA 偵測出不同的 topics\n",
    "3. 輸出 topic, words, 對應的 node schema 檔\n",
    "4. 利用這些 topics 建立 taxonomy tree\n",
    "5. 專注於 n8n nodes 的功能分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent\n",
      "Working directory: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent\n",
      "Node schemas path: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/node_schemas\n",
      "Found 792 JSON schema files\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 設定專案路徑\n",
    "project_root = Path().absolute()\n",
    "node_schemas_path = project_root / \"node_schemas\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {Path().absolute()}\")\n",
    "print(f\"Node schemas path: {node_schemas_path}\")\n",
    "\n",
    "# 檢查 node_schemas 目錄是否存在\n",
    "if not node_schemas_path.exists():\n",
    "    print(f\"Error: {node_schemas_path} does not exist!\")\n",
    "else:\n",
    "    schema_files = list(node_schemas_path.glob(\"*.json\"))\n",
    "    print(f\"Found {len(schema_files)} JSON schema files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeSchemaParser class created successfully!\n"
     ]
    }
   ],
   "source": [
    "class NodeSchemaParser:\n",
    "    def __init__(self, schemas_dir):\n",
    "        self.schemas_dir = Path(schemas_dir)\n",
    "    \n",
    "    def extract_text_from_json(self, data, exclude_keys=None):\n",
    "        if exclude_keys is None:\n",
    "            exclude_keys = {'type', 'required', 'default', 'noDataExpression'}\n",
    "        \n",
    "        texts = []\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if key in exclude_keys:\n",
    "                    continue\n",
    "                if isinstance(value, str) and len(value.strip()) > 1:\n",
    "                    texts.append(value.strip())\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    texts.extend(self.extract_text_from_json(value, exclude_keys))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                texts.extend(self.extract_text_from_json(item, exclude_keys))\n",
    "        elif isinstance(data, str) and len(data.strip()) > 1:\n",
    "            texts.append(data.strip())\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def parse_schema_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                schema_data = json.load(f)\n",
    "            \n",
    "            name = schema_data.get('name', file_path.stem)\n",
    "            display_name = schema_data.get('displayName', '')\n",
    "            description = schema_data.get('description', '')\n",
    "            title = display_name if display_name else name\n",
    "            \n",
    "            all_texts = self.extract_text_from_json(schema_data)\n",
    "            content = ' '.join(all_texts)\n",
    "            \n",
    "            return {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'name': name,\n",
    "                'display_name': display_name,\n",
    "                'description': description,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'raw_data': schema_data\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_all_schemas(self):\n",
    "        schemas = {}\n",
    "        schema_files = list(self.schemas_dir.glob(\"*.json\"))\n",
    "        \n",
    "        for file_path in schema_files:\n",
    "            schema_info = self.parse_schema_file(file_path)\n",
    "            if schema_info:\n",
    "                schemas[schema_info['file_name']] = schema_info\n",
    "        \n",
    "        return schemas\n",
    "\n",
    "print(\"NodeSchemaParser class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicModeler class created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        self.n_topics = n_topics\n",
    "        self.random_state = random_state\n",
    "        self.vectorizer = None\n",
    "        self.lda_model = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        processed_docs = [self.preprocess_text(doc) for doc in documents]\n",
    "        processed_docs = [doc for doc in processed_docs if len(doc.strip()) > 0]\n",
    "        \n",
    "        self.vectorizer = CountVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        doc_term_matrix = self.vectorizer.fit_transform(processed_docs)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=self.n_topics,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=10,\n",
    "            learning_method='online'\n",
    "        )\n",
    "        \n",
    "        doc_topic_probs = self.lda_model.fit_transform(doc_term_matrix)\n",
    "        perplexity = self.lda_model.perplexity(doc_term_matrix)\n",
    "        \n",
    "        results = self._generate_results(doc_topic_probs)\n",
    "        results['model_perplexity'] = perplexity\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_results(self, doc_topic_probs):\n",
    "        results = {\n",
    "            'topics': [],\n",
    "            'document_topics': [],\n",
    "            'topic_similarities': []\n",
    "        }\n",
    "        \n",
    "        for topic_idx in range(self.n_topics):\n",
    "            topic_words = []\n",
    "            topic_weights = self.lda_model.components_[topic_idx]\n",
    "            top_indices = topic_weights.argsort()[-10:][::-1]\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                topic_words.append({\n",
    "                    'word': self.feature_names[idx],\n",
    "                    'weight': float(topic_weights[idx])\n",
    "                })\n",
    "            \n",
    "            top_words = [w['word'] for w in topic_words[:3]]\n",
    "            label = ' + '.join(top_words).title()\n",
    "            coherence = np.mean([w['weight'] for w in topic_words[:5]])\n",
    "            \n",
    "            results['topics'].append({\n",
    "                'id': topic_idx,\n",
    "                'label': label,\n",
    "                'top_words': topic_words,\n",
    "                'coherence': float(coherence)\n",
    "            })\n",
    "        \n",
    "        for doc_idx, topic_probs in enumerate(doc_topic_probs):\n",
    "            dominant_topic = int(np.argmax(topic_probs))\n",
    "            dominant_prob = float(np.max(topic_probs))\n",
    "            \n",
    "            results['document_topics'].append({\n",
    "                'document_index': doc_idx,\n",
    "                'dominant_topic': dominant_topic,\n",
    "                'dominant_topic_probability': dominant_prob,\n",
    "                'all_topic_probabilities': topic_probs.tolist()\n",
    "            })\n",
    "        \n",
    "        topic_similarities = np.corrcoef(self.lda_model.components_)\n",
    "        results['topic_similarities'] = topic_similarities.tolist()\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"TopicModeler class created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 載入和解析 Node Schema 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 792 schema files\n",
      "First few files: ['onfleet.json', 'lemlistTool.json', 'chat.json', 'wait.json', 'crowdDevTool.json']\n",
      "\n",
      "Example schema: onfleet.json\n",
      "Display name: Onfleet\n",
      "Description: Consume Onfleet API...\n",
      "Content length: 22165 characters\n",
      "Title: Onfleet\n"
     ]
    }
   ],
   "source": [
    "# 解析 Node Schema 檔案\n",
    "parser = NodeSchemaParser(node_schemas_path)\n",
    "schemas = parser.parse_all_schemas()\n",
    "\n",
    "print(f\"Successfully parsed {len(schemas)} schema files\")\n",
    "print(f\"First few files: {list(schemas.keys())[:5]}\")\n",
    "\n",
    "# 顯示範例 schema 內容\n",
    "if schemas:\n",
    "    first_schema = next(iter(schemas.values()))\n",
    "    print(f\"\\nExample schema: {first_schema['file_name']}\")\n",
    "    print(f\"Display name: {first_schema['display_name']}\")\n",
    "    print(f\"Description: {first_schema['description'][:100]}...\")\n",
    "    print(f\"Content length: {len(first_schema['content'])} characters\")\n",
    "    print(f\"Title: {first_schema['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 準備文件內容進行 LDA 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 792 documents for LDA analysis\n",
      "Content length - Min: 131, Max: 136209, Mean: 6732\n"
     ]
    }
   ],
   "source": [
    "# 準備文件內容\n",
    "documents = []\n",
    "file_mapping = []\n",
    "\n",
    "for file_path, schema_data in schemas.items():\n",
    "    content = schema_data[\"content\"]\n",
    "    if len(content.strip()) > 50:\n",
    "        documents.append(content)\n",
    "        file_mapping.append({\n",
    "            \"file_path\": schema_data[\"file_path\"],\n",
    "            \"file_name\": schema_data[\"file_name\"],\n",
    "            \"title\": schema_data[\"title\"],\n",
    "            \"display_name\": schema_data[\"display_name\"],\n",
    "            \"description\": schema_data[\"description\"]\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(documents)} documents for LDA analysis\")\n",
    "\n",
    "content_lengths = [len(doc) for doc in documents]\n",
    "print(f\"Content length - Min: {min(content_lengths)}, Max: {max(content_lengths)}, Mean: {np.mean(content_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 執行 LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA analysis completed!\n",
      "Generated 10 topics\n",
      "Model perplexity: 264.39\n"
     ]
    }
   ],
   "source": [
    "# 執行 LDA\n",
    "topic_modeler = TopicModeler()\n",
    "lda_results = topic_modeler.fit_transform(documents)\n",
    "\n",
    "print(f\"LDA analysis completed!\")\n",
    "print(f\"Generated {len(lda_results['topics'])} topics\")\n",
    "print(f\"Model perplexity: {lda_results.get('model_perplexity', 'N/A'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 顯示 Topics 和對應的檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Topic 0: Contact + Id + Field ===\n",
      "Coherence Score: 1137.054\n",
      "\n",
      "Top Words:\n",
      "  contact              1751.1399\n",
      "  id                   1062.6179\n",
      "  field                1012.8356\n",
      "  dollar               1007.0941\n",
      "  email                851.5806\n",
      "  company              807.1024\n",
      "  custom               779.6133\n",
      "  fields               731.5947\n",
      "  address              652.3607\n",
      "  update               643.7257\n",
      "\n",
      "Files in this topic (38 files with prob >= 0.2):\n",
      "  agileCrm.json                  (0.988)\n",
      "  clearbit.json                  (0.965)\n",
      "  zohoCrm.json                   (0.906)\n",
      "  zohoCrmTool.json               (0.905)\n",
      "  activeCampaign.json            (0.876)\n",
      "  agileCrmTool.json              (0.867)\n",
      "  drift.json                     (0.859)\n",
      "  uplead.json                    (0.857)\n",
      "  activeCampaignTool.json        (0.840)\n",
      "  mailcheck.json                 (0.726)\n",
      "  sendInBlueTrigger.json         (0.714)\n",
      "  googleContacts.json            (0.656)\n",
      "  sort.json                      (0.638)\n",
      "  driftTool.json                 (0.605)\n",
      "  Brandfetch.json                (0.600)\n",
      "  ... and 23 more files\n",
      "\n",
      "=== Topic 1: Id + User + Create ===\n",
      "Coherence Score: 2618.446\n",
      "\n",
      "Top Words:\n",
      "  id                   3657.4041\n",
      "  user                 2829.0132\n",
      "  create               2743.3792\n",
      "  update               2212.3568\n",
      "  delete               1650.0759\n",
      "  list                 1624.8572\n",
      "  getall               1566.1134\n",
      "  return               1394.2086\n",
      "  fields               1242.5840\n",
      "  limit                1218.1836\n",
      "\n",
      "Files in this topic (113 files with prob >= 0.2):\n",
      "  discourse.json                 (0.998)\n",
      "  strava.json                    (0.950)\n",
      "  trelloTool.json                (0.945)\n",
      "  trello.json                    (0.941)\n",
      "  discourseTool.json             (0.939)\n",
      "  sendy.json                     (0.910)\n",
      "  reddit.json                    (0.894)\n",
      "  iterable.json                  (0.890)\n",
      "  flow.json                      (0.874)\n",
      "  philipsHue.json                (0.871)\n",
      "  flowTrigger.json               (0.855)\n",
      "  netlify.json                   (0.849)\n",
      "  mailerLiteTrigger.json         (0.846)\n",
      "  bitwarden.json                 (0.837)\n",
      "  crowdDev.json                  (0.822)\n",
      "  ... and 98 more files\n",
      "\n",
      "=== Topic 2: America + Utc + Asia ===\n",
      "Coherence Score: 2249.232\n",
      "\n",
      "Top Words:\n",
      "  america              3309.1713\n",
      "  utc                  2852.3001\n",
      "  asia                 1943.6474\n",
      "  00                   1873.2573\n",
      "  europe               1267.7855\n",
      "  africa               1069.1582\n",
      "  pacific              895.3252\n",
      "  contact              818.6865\n",
      "  id                   685.0650\n",
      "  company              564.5723\n",
      "\n",
      "Files in this topic (6 files with prob >= 0.2):\n",
      "  googleCalendar.json            (0.989)\n",
      "  googleCalendarTool.json        (0.957)\n",
      "  freshworksCrm.json             (0.763)\n",
      "  freshworksCrmTool.json         (0.761)\n",
      "  hubspotTool.json               (0.593)\n",
      "  microsoftOutlookTool.json      (0.465)\n",
      "\n",
      "=== Topic 3: Field + Id + 9A ===\n",
      "Coherence Score: 1667.517\n",
      "\n",
      "Top Words:\n",
      "  field                2021.1398\n",
      "  id                   1983.5762\n",
      "  9a                   1628.3841\n",
      "  case                 1491.7951\n",
      "  channel              1212.6901\n",
      "  9a 9a                1157.1845\n",
      "  https                1147.3616\n",
      "  list                 1088.1520\n",
      "  text                 1057.6159\n",
      "  date                 1020.5020\n",
      "\n",
      "Files in this topic (9 files with prob >= 0.2):\n",
      "  theHiveProject.json            (0.754)\n",
      "  theHiveProjectTool.json        (0.737)\n",
      "  notion.json                    (0.640)\n",
      "  notionTool.json                (0.635)\n",
      "  stickyNote.json                (0.631)\n",
      "  microsoftTeams.json            (0.572)\n",
      "  microsoftTeamsTrigger.json     (0.538)\n",
      "  microsoftTeamsTool.json        (0.537)\n",
      "  slackTool.json                 (0.352)\n",
      "\n",
      "=== Topic 4: File + Message + Send ===\n",
      "Coherence Score: 1448.925\n",
      "\n",
      "Top Words:\n",
      "  file                 2451.6695\n",
      "  message              1871.6677\n",
      "  send                 1046.1509\n",
      "  options              944.0800\n",
      "  field                931.0583\n",
      "  text                 849.4014\n",
      "  binary               808.4089\n",
      "  url                  728.5953\n",
      "  data                 709.6837\n",
      "  folder               655.0368\n",
      "\n",
      "Files in this topic (194 files with prob >= 0.2):\n",
      "  googleGeminiTool.json          (0.999)\n",
      "  googleGemini.json              (0.999)\n",
      "  formTrigger.json               (0.997)\n",
      "  ftp.json                       (0.995)\n",
      "  messageBird.json               (0.993)\n",
      "  ssh.json                       (0.993)\n",
      "  sms77.json                     (0.993)\n",
      "  compression.json               (0.992)\n",
      "  anthropic.json                 (0.991)\n",
      "  mocean.json                    (0.986)\n",
      "  yourls.json                    (0.981)\n",
      "  msg91.json                     (0.980)\n",
      "  readWriteFile.json             (0.977)\n",
      "  discord.json                   (0.975)\n",
      "  mqtt.json                      (0.974)\n",
      "  ... and 179 more files\n",
      "\n",
      "=== Topic 5: Id + List + Expressions ===\n",
      "Coherence Score: 4561.897\n",
      "\n",
      "Top Words:\n",
      "  id                   7481.1514\n",
      "  list                 4475.4359\n",
      "  expressions          3645.7049\n",
      "  using                3605.7389\n",
      "  specify              3601.4551\n",
      "  expression           3567.6406\n",
      "  using href           3550.1030\n",
      "  choose list          3538.9472\n",
      "  docs n8n             3535.8091\n",
      "  io code              3527.5139\n",
      "\n",
      "Files in this topic (111 files with prob >= 0.2):\n",
      "  wekanTool.json                 (1.000)\n",
      "  wekan.json                     (1.000)\n",
      "  linear.json                    (0.998)\n",
      "  zendeskTrigger.json            (0.998)\n",
      "  venafiTlsProtectCloudTrigger.json (0.965)\n",
      "  taiga.json                     (0.904)\n",
      "  taigaTool.json                 (0.893)\n",
      "  workableTrigger.json           (0.872)\n",
      "  payPalTrigger.json             (0.833)\n",
      "  linearTool.json                (0.819)\n",
      "  erpNext.json                   (0.815)\n",
      "  webflow.json                   (0.800)\n",
      "  freshservice.json              (0.798)\n",
      "  bitbucketTrigger.json          (0.794)\n",
      "  taigaTrigger.json              (0.792)\n",
      "  ... and 96 more files\n",
      "\n",
      "=== Topic 6: Create + Id + Address ===\n",
      "Coherence Score: 833.277\n",
      "\n",
      "Top Words:\n",
      "  create               1122.4637\n",
      "  id                   1053.5173\n",
      "  address              728.0308\n",
      "  update               637.1029\n",
      "  customer             625.2684\n",
      "  number               557.2910\n",
      "  date                 541.1826\n",
      "  getall               541.1414\n",
      "  key                  530.7236\n",
      "  task                 512.3508\n",
      "\n",
      "Files in this topic (30 files with prob >= 0.2):\n",
      "  stripeTool.json                (0.971)\n",
      "  stripe.json                    (0.963)\n",
      "  wooCommerceTool.json           (0.888)\n",
      "  onfleetTool.json               (0.886)\n",
      "  onfleet.json                   (0.879)\n",
      "  wooCommerce.json               (0.878)\n",
      "  quickbooks.json                (0.858)\n",
      "  quickbooksTool.json            (0.857)\n",
      "  onfleetTrigger.json            (0.746)\n",
      "  unleashedSoftware.json         (0.687)\n",
      "  shopify.json                   (0.679)\n",
      "  shopifyTool.json               (0.666)\n",
      "  marketstack.json               (0.657)\n",
      "  invoiceNinjaTool.json          (0.637)\n",
      "  invoiceNinja.json              (0.632)\n",
      "  ... and 15 more files\n",
      "\n",
      "=== Topic 7: Database + Text + Data ===\n",
      "Coherence Score: 701.121\n",
      "\n",
      "Top Words:\n",
      "  database             996.8126\n",
      "  text                 830.2088\n",
      "  data                 707.1998\n",
      "  value                519.2690\n",
      "  condition            452.1150\n",
      "  notion               445.2124\n",
      "  id                   428.9631\n",
      "  row                  418.2138\n",
      "  workflow             408.1341\n",
      "  table                387.5364\n",
      "\n",
      "Files in this topic (117 files with prob >= 0.2):\n",
      "  merge.json                     (0.995)\n",
      "  scheduleTrigger.json           (0.991)\n",
      "  cron.json                      (0.990)\n",
      "  togglTrigger.json              (0.989)\n",
      "  rssFeedReadTrigger.json        (0.989)\n",
      "  documentJsonInputLoader.json   (0.982)\n",
      "  vectorStoreZepLoad.json        (0.977)\n",
      "  vectorStorePineconeLoad.json   (0.976)\n",
      "  vectorStoreInMemoryLoad.json   (0.959)\n",
      "  toolCalculator.json            (0.957)\n",
      "  toolWikipedia.json             (0.955)\n",
      "  toolWolframAlpha.json          (0.953)\n",
      "  function.json                  (0.950)\n",
      "  functionItem.json              (0.947)\n",
      "  start.json                     (0.944)\n",
      "  ... and 102 more files\n",
      "\n",
      "=== Topic 8: _Blank + Target _Blank + App ===\n",
      "Coherence Score: 783.761\n",
      "\n",
      "Top Words:\n",
      "  _blank               788.8929\n",
      "  target _blank        788.3912\n",
      "  app                  787.8194\n",
      "  target               785.4565\n",
      "  https                768.2469\n",
      "  href                 754.1308\n",
      "  href https           753.1832\n",
      "  tools                752.7316\n",
      "  https app            733.9565\n",
      "  info                 725.6598\n",
      "\n",
      "Files in this topic (2 files with prob >= 0.2):\n",
      "  uprocTool.json                 (1.000)\n",
      "  uproc.json                     (1.000)\n",
      "\n",
      "=== Topic 9: Operation + Set + Id ===\n",
      "Coherence Score: 761.116\n",
      "\n",
      "Top Words:\n",
      "  operation            953.9817\n",
      "  set                  751.1729\n",
      "  id                   725.8734\n",
      "  update               693.9250\n",
      "  value                680.6261\n",
      "  return               662.3623\n",
      "  create               635.2127\n",
      "  list                 633.9112\n",
      "  description          622.4938\n",
      "  api                  597.4068\n",
      "\n",
      "Files in this topic (172 files with prob >= 0.2):\n",
      "  npmTool.json                   (0.996)\n",
      "  npm.json                       (0.994)\n",
      "  stackbyTool.json               (0.994)\n",
      "  snowflakeTool.json             (0.994)\n",
      "  rundeckTool.json               (0.993)\n",
      "  googleBusinessProfileTool.json (0.989)\n",
      "  googleBusinessProfile.json     (0.987)\n",
      "  elasticsearchTool.json         (0.985)\n",
      "  rundeck.json                   (0.985)\n",
      "  metabaseTool.json              (0.982)\n",
      "  executeCommandTool.json        (0.975)\n",
      "  azureCosmosDb.json             (0.974)\n",
      "  mongoDbTool.json               (0.968)\n",
      "  crateDbTool.json               (0.967)\n",
      "  metabase.json                  (0.964)\n",
      "  ... and 157 more files\n"
     ]
    }
   ],
   "source": [
    "def display_topic_details(topic_id, min_probability=0.2):\n",
    "    topic_info = lda_results['topics'][topic_id]\n",
    "    \n",
    "    print(f\"\\n=== Topic {topic_id}: {topic_info['label']} ===\")\n",
    "    print(f\"Coherence Score: {topic_info['coherence']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop Words:\")\n",
    "    for word_info in topic_info['top_words'][:10]:\n",
    "        print(f\"  {word_info['word']: <20} {word_info['weight']:.4f}\")\n",
    "    \n",
    "    topic_files = []\n",
    "    for doc_idx, doc_topic in enumerate(lda_results['document_topics']):\n",
    "        if doc_topic['dominant_topic'] == topic_id and doc_topic['dominant_topic_probability'] >= min_probability:\n",
    "            file_info = file_mapping[doc_idx]\n",
    "            topic_files.append({\n",
    "                **file_info,\n",
    "                'probability': doc_topic['dominant_topic_probability']\n",
    "            })\n",
    "    \n",
    "    topic_files.sort(key=lambda x: x['probability'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nFiles in this topic ({len(topic_files)} files with prob >= {min_probability}):\")\n",
    "    for file_info in topic_files[:15]:\n",
    "        print(f\"  {file_info['file_name']: <30} ({file_info['probability']:.3f})\")\n",
    "    \n",
    "    if len(topic_files) > 15:\n",
    "        print(f\"  ... and {len(topic_files) - 15} more files\")\n",
    "    \n",
    "    return topic_files\n",
    "\n",
    "# 顯示所有 topics\n",
    "all_topic_files = {}\n",
    "for i in range(len(lda_results['topics'])):\n",
    "    topic_files = display_topic_details(i)\n",
    "    all_topic_files[i] = topic_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 建立 LDA-Based Taxonomy Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA-Based Taxonomy Tree created successfully!\n",
      "Classification Statistics:\n",
      "  total_files: 792\n",
      "  classified_files: 792\n",
      "  unclassified_files: 0\n",
      "  classification_rate: 100.00\n",
      "  topics_count: 10\n",
      "  avg_files_per_topic: 79.20\n"
     ]
    }
   ],
   "source": [
    "class LDABasedTaxonomyBuilder:\n",
    "    def __init__(self, lda_results, file_mapping, min_probability=0.15):\n",
    "        self.lda_results = lda_results\n",
    "        self.file_mapping = file_mapping\n",
    "        self.min_probability = min_probability\n",
    "        self.taxonomy_tree = {}\n",
    "    \n",
    "    def build_taxonomy(self):\n",
    "        self.taxonomy_tree = {\n",
    "            \"metadata\": {\n",
    "                \"method\": \"LDA_Based_Taxonomy_n8n_Nodes\",\n",
    "                \"data_source\": \"n8n_node_schemas\",\n",
    "                \"n_topics\": len(self.lda_results['topics']),\n",
    "                \"min_probability\": self.min_probability,\n",
    "                \"total_files\": len(self.file_mapping),\n",
    "                \"model_perplexity\": self.lda_results.get('model_perplexity', 'N/A')\n",
    "            },\n",
    "            \"topics\": {},\n",
    "            \"unclassified\": {\n",
    "                \"files\": [],\n",
    "                \"description\": \"Files that don't meet the minimum probability threshold\"\n",
    "            },\n",
    "            \"statistics\": {}\n",
    "        }\n",
    "        \n",
    "        classified_files = set()\n",
    "        \n",
    "        for topic_id, topic_info in enumerate(self.lda_results['topics']):\n",
    "            topic_key = f\"topic_{topic_id}\"\n",
    "            topic_files = []\n",
    "            \n",
    "            for doc_idx, doc_topic in enumerate(self.lda_results['document_topics']):\n",
    "                if (doc_topic['dominant_topic'] == topic_id and \n",
    "                    doc_topic['dominant_topic_probability'] >= self.min_probability):\n",
    "                    \n",
    "                    file_info = self.file_mapping[doc_idx]\n",
    "                    topic_files.append({\n",
    "                        \"file_path\": file_info['file_path'],\n",
    "                        \"file_name\": file_info['file_name'],\n",
    "                        \"display_name\": file_info['display_name'],\n",
    "                        \"description\": file_info['description'],\n",
    "                        \"title\": file_info['title'],\n",
    "                        \"probability\": doc_topic['dominant_topic_probability'],\n",
    "                        \"all_topic_probs\": doc_topic['all_topic_probabilities']\n",
    "                    })\n",
    "                    \n",
    "                    classified_files.add(doc_idx)\n",
    "            \n",
    "            topic_files.sort(key=lambda x: x['probability'], reverse=True)\n",
    "            \n",
    "            self.taxonomy_tree[\"topics\"][topic_key] = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"label\": topic_info['label'],\n",
    "                \"description\": f\"n8n nodes strongly associated with {topic_info['label']}\",\n",
    "                \"files\": topic_files,\n",
    "                \"top_words\": [w['word'] for w in topic_info['top_words'][:10]],\n",
    "                \"coherence\": topic_info['coherence'],\n",
    "                \"file_count\": len(topic_files),\n",
    "                \"avg_probability\": np.mean([f['probability'] for f in topic_files]) if topic_files else 0\n",
    "            }\n",
    "        \n",
    "        unclassified_files = []\n",
    "        for doc_idx, file_info in enumerate(self.file_mapping):\n",
    "            if doc_idx not in classified_files:\n",
    "                doc_topic = self.lda_results['document_topics'][doc_idx]\n",
    "                unclassified_files.append({\n",
    "                    \"file_path\": file_info['file_path'],\n",
    "                    \"file_name\": file_info['file_name'],\n",
    "                    \"display_name\": file_info['display_name'],\n",
    "                    \"description\": file_info['description'],\n",
    "                    \"title\": file_info['title'],\n",
    "                    \"max_probability\": max(doc_topic['all_topic_probabilities']),\n",
    "                    \"dominant_topic\": doc_topic['dominant_topic'],\n",
    "                    \"reason\": \"Below minimum probability threshold\"\n",
    "                })\n",
    "        \n",
    "        self.taxonomy_tree[\"unclassified\"][\"files\"] = unclassified_files\n",
    "        self._generate_statistics()\n",
    "        \n",
    "        return self.taxonomy_tree\n",
    "    \n",
    "    def _generate_statistics(self):\n",
    "        total_files = len(self.file_mapping)\n",
    "        classified_files = sum(topic['file_count'] for topic in self.taxonomy_tree['topics'].values())\n",
    "        unclassified_files = len(self.taxonomy_tree['unclassified']['files'])\n",
    "        \n",
    "        self.taxonomy_tree['statistics'] = {\n",
    "            \"total_files\": total_files,\n",
    "            \"classified_files\": classified_files,\n",
    "            \"unclassified_files\": unclassified_files,\n",
    "            \"classification_rate\": (classified_files / total_files) * 100,\n",
    "            \"topics_count\": len(self.taxonomy_tree['topics']),\n",
    "            \"avg_files_per_topic\": classified_files / len(self.taxonomy_tree['topics']) if self.taxonomy_tree['topics'] else 0\n",
    "        }\n",
    "\n",
    "# 建立 LDA-based taxonomy\n",
    "taxonomy_builder = LDABasedTaxonomyBuilder(lda_results, file_mapping, min_probability=0.15)\n",
    "lda_taxonomy = taxonomy_builder.build_taxonomy()\n",
    "\n",
    "print(\"LDA-Based Taxonomy Tree created successfully!\")\n",
    "print(f\"Classification Statistics:\")\n",
    "for key, value in lda_taxonomy['statistics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 匯出結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ n8n node taxonomy tree exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_taxonomy_tree.json\n",
      "📄 Simplified taxonomy summary exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_taxonomy_summary.json\n",
      "📊 Topic-file mapping exported to: /Users/yu/Desktop/projects/gss_cai/n8n_AI_Agent/data/output/n8n_node_topic_mapping.csv\n",
      "\n",
      "✅ n8n Node Schema LDA Analysis Complete!\n",
      "📊 Generated 10 topic-based categories\n",
      "📁 Classified 792/792 node schemas (100.0%)\n",
      "📈 Average node schemas per topic: 79.2\n"
     ]
    }
   ],
   "source": [
    "# 匯出結果\n",
    "output_dir = project_root / \"data\" / \"output\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_json_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (bool, int, float, str)) or obj is None:\n",
    "        return obj\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "final_taxonomy = make_json_serializable(lda_taxonomy)\n",
    "\n",
    "# 匯出完整的 taxonomy tree\n",
    "taxonomy_output_path = output_dir / \"n8n_node_taxonomy_tree.json\"\n",
    "with open(taxonomy_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_taxonomy, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ n8n node taxonomy tree exported to: {taxonomy_output_path}\")\n",
    "\n",
    "# 匯出簡化版本\n",
    "simplified_taxonomy = {\n",
    "    \"summary\": final_taxonomy['statistics'],\n",
    "    \"metadata\": final_taxonomy['metadata'],\n",
    "    \"topics\": {\n",
    "        topic_key: {\n",
    "            \"label\": topic_data['label'],\n",
    "            \"file_count\": topic_data['file_count'],\n",
    "            \"top_words\": topic_data['top_words'][:5],\n",
    "            \"top_files\": [f['file_name'] for f in topic_data['files'][:5]]\n",
    "        }\n",
    "        for topic_key, topic_data in final_taxonomy['topics'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "simplified_output_path = output_dir / \"n8n_node_taxonomy_summary.json\"\n",
    "with open(simplified_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_taxonomy, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📄 Simplified taxonomy summary exported to: {simplified_output_path}\")\n",
    "\n",
    "# 匯出 CSV 對應表\n",
    "topic_file_mapping = []\n",
    "for topic_key, topic_data in final_taxonomy['topics'].items():\n",
    "    for file_info in topic_data['files']:\n",
    "        topic_file_mapping.append({\n",
    "            'topic_id': topic_data['topic_id'],\n",
    "            'topic_label': topic_data['label'],\n",
    "            'file_name': file_info['file_name'],\n",
    "            'display_name': file_info['display_name'],\n",
    "            'probability': file_info['probability']\n",
    "        })\n",
    "\n",
    "df_mapping = pd.DataFrame(topic_file_mapping)\n",
    "csv_output_path = output_dir / \"n8n_node_topic_mapping.csv\"\n",
    "df_mapping.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "print(f\"📊 Topic-file mapping exported to: {csv_output_path}\")\n",
    "\n",
    "print(f\"\\n✅ n8n Node Schema LDA Analysis Complete!\")\n",
    "print(f\"📊 Generated {final_taxonomy['statistics']['topics_count']} topic-based categories\")\n",
    "print(f\"📁 Classified {final_taxonomy['statistics']['classified_files']}/{final_taxonomy['statistics']['total_files']} node schemas ({final_taxonomy['statistics']['classification_rate']:.1f}%)\")\n",
    "print(f\"📈 Average node schemas per topic: {final_taxonomy['statistics']['avg_files_per_topic']:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
