#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è¼‰å„ªé¸çš„ n8n ç¤¾ç¾¤ç¯€é»å¥—ä»¶
åŸºæ–¼ä¹‹å‰çš„æœå°‹çµæœï¼Œä¸‹è¼‰æœ€æœ‰åƒ¹å€¼çš„å¥—ä»¶
"""

import requests
import tarfile
import json
import time
from pathlib import Path

def download_priority_packages():
    """ä¸‹è¼‰å„ªå…ˆé¸æ“‡çš„ç¤¾ç¾¤å¥—ä»¶"""
    
    # æ ¹æ“šæœå°‹çµæœé¸æ“‡çš„é«˜åƒ¹å€¼å¥—ä»¶
    priority_packages = [
        "@apify/n8n-nodes-apify",  # Apify ç¶²é çˆ¬èŸ²
        "@elevenlabs/n8n-nodes-elevenlabs",  # ElevenLabs AI èªéŸ³
        "@bitovi/n8n-nodes-excel",  # Excel æª”æ¡ˆè™•ç†
        "@bitovi/n8n-nodes-google-search",  # Google æœå°‹
        "@cloudconvert/n8n-nodes-cloudconvert",  # æª”æ¡ˆè½‰æ›
        "@devlikeapro/n8n-nodes-chatwoot",  # Chatwoot å®¢æœ
        "@devlikeapro/n8n-nodes-waha",  # WhatsApp API
        "@formbricks/n8n-nodes-formbricks",  # è¡¨å–®å»ºæ§‹
        "@firefliesai/n8n-nodes-fireflies",  # æœƒè­° AI è¨˜éŒ„
        "@digital-boss/n8n-nodes-pdf-merge",  # PDF è™•ç†
        "@brave/n8n-nodes-brave-search",  # Brave æœå°‹å¼•æ“
        "n8n-nodes-puppeteer",  # Puppeteer è‡ªå‹•åŒ–
        "n8n-nodes-azure-cosmos-db",  # Azure Cosmos DB
        "n8n-nodes-mongodb",  # MongoDB
        "n8n-nodes-jsonata",  # JSONata è³‡æ–™è½‰æ›
    ]
    
    print(f"ğŸ“¦ é–‹å§‹ä¸‹è¼‰ {len(priority_packages)} å€‹å„ªé¸å¥—ä»¶...")
    
    output_dir = Path("additional_community_packages")
    output_dir.mkdir(exist_ok=True)
    
    successful_downloads = []
    failed_downloads = []
    
    for i, package_name in enumerate(priority_packages):
        try:
            print(f"\nä¸‹è¼‰ {i+1}/{len(priority_packages)}: {package_name}")
            
            # ç²å–å¥—ä»¶è³‡è¨Š
            url = f"https://registry.npmjs.org/{package_name}"
            response = requests.get(url, timeout=10)
            
            if response.status_code != 200:
                print(f"  âŒ å¥—ä»¶ä¸å­˜åœ¨æˆ–ç„¡æ³•è¨ªå•")
                failed_downloads.append(package_name)
                continue
            
            data = response.json()
            latest_version = data.get('dist-tags', {}).get('latest', '')
            
            if not latest_version:
                print(f"  âŒ ç„¡æ³•ç²å–ç‰ˆæœ¬è³‡è¨Š")
                failed_downloads.append(package_name)
                continue
            
            # ç²å– tarball URL
            version_data = data['versions'][latest_version]
            tarball_url = version_data['dist']['tarball']
            
            print(f"  ç‰ˆæœ¬: {latest_version}")
            print(f"  ä¸‹è¼‰: {tarball_url}")
            
            # ä¸‹è¼‰ tarball
            response = requests.get(tarball_url, timeout=30)
            
            if response.status_code == 200:
                # å„²å­˜æª”æ¡ˆ
                safe_name = package_name.replace('/', '_').replace('@', '')
                tarball_path = output_dir / f"{safe_name}-{latest_version}.tgz"
                
                with open(tarball_path, 'wb') as f:
                    f.write(response.content)
                
                # è§£å£“ç¸®
                extract_dir = output_dir / safe_name
                extract_dir.mkdir(exist_ok=True)
                
                with tarfile.open(tarball_path, 'r:gz') as tar:
                    tar.extractall(extract_dir)
                
                successful_downloads.append({
                    'name': package_name,
                    'version': latest_version,
                    'description': version_data.get('description', ''),
                    'path': str(extract_dir)
                })
                
                print(f"  âœ… ä¸‹è¼‰æˆåŠŸ")
            else:
                print(f"  âŒ ä¸‹è¼‰å¤±æ•—: HTTP {response.status_code}")
                failed_downloads.append(package_name)
                
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            failed_downloads.append(package_name)
        
        time.sleep(1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
    
    return successful_downloads, failed_downloads

def analyze_new_packages(downloaded_packages):
    """åˆ†ææ–°ä¸‹è¼‰çš„å¥—ä»¶"""
    
    print(f"\nğŸ” åˆ†æ {len(downloaded_packages)} å€‹æ–°ä¸‹è¼‰çš„å¥—ä»¶...")
    
    total_nodes = 0
    total_js_files = 0
    package_analysis = []
    
    for pkg in downloaded_packages:
        try:
            pkg_path = Path(pkg['path'])
            
            # æœå°‹ .node.js æª”æ¡ˆ
            node_files = list(pkg_path.glob("**/*.node.js"))
            js_files = list(pkg_path.glob("**/*.js"))
            
            analysis = {
                'name': pkg['name'],
                'version': pkg['version'],
                'description': pkg['description'],
                'node_files': len(node_files),
                'total_js_files': len(js_files),
                'node_list': [f.name for f in node_files]
            }
            
            package_analysis.append(analysis)
            total_nodes += len(node_files)
            total_js_files += len(js_files)
            
            print(f"  ğŸ“¦ {pkg['name']}")
            print(f"     ç¯€é»: {len(node_files)} å€‹")
            print(f"     JSæª”æ¡ˆ: {len(js_files)} å€‹")
            
            if node_files:
                print(f"     ç¯€é»åˆ—è¡¨: {', '.join([f.name for f in node_files[:3]])}{'...' if len(node_files) > 3 else ''}")
            
        except Exception as e:
            print(f"  âŒ åˆ†æ {pkg['name']} å¤±æ•—: {e}")
    
    print(f"\nğŸ“Š çµ±è¨ˆçµæœ:")
    print(f"  æˆåŠŸä¸‹è¼‰çš„å¥—ä»¶: {len(downloaded_packages)}")
    print(f"  ç¸½ç¯€é»æª”æ¡ˆ: {total_nodes}")
    print(f"  ç¸½ JS æª”æ¡ˆ: {total_js_files}")
    
    return package_analysis

def extract_schemas_from_new_packages():
    """å¾æ–°å¥—ä»¶ä¸­æå– schema"""
    
    print(f"\nğŸ”§ å¾æ–°å¥—ä»¶ä¸­æå–ç¯€é» schema...")
    
    base_dir = Path("additional_community_packages")
    output_dir = Path("additional_node_schemas")
    output_dir.mkdir(exist_ok=True)
    
    # æœå°‹æ‰€æœ‰ .node.js æª”æ¡ˆ
    node_files = list(base_dir.glob("**/*.node.js"))
    
    print(f"  æ‰¾åˆ° {len(node_files)} å€‹ç¯€é»æª”æ¡ˆ")
    
    extracted_schemas = []
    
    for node_file in node_files:
        try:
            with open(node_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç°¡å–®æå–ç¯€é»è³‡è¨Š
            node_info = {}
            
            # æå– displayName
            if "displayName:" in content:
                lines = content.split('\n')
                for line in lines:
                    if 'displayName:' in line and ("'" in line or '"' in line):
                        quote_char = "'" if "'" in line else '"'
                        start = line.find(quote_char) + 1
                        end = line.find(quote_char, start)
                        if end > start:
                            node_info['displayName'] = line[start:end]
                            break
            
            # æå– name
            if "name:" in content:
                lines = content.split('\n')
                for line in lines:
                    if 'name:' in line and ("'" in line or '"' in line):
                        quote_char = "'" if "'" in line else '"'
                        start = line.find(quote_char) + 1
                        end = line.find(quote_char, start)
                        if end > start:
                            node_info['name'] = line[start:end]
                            break
            
            # æå– description
            if "description:" in content:
                lines = content.split('\n')
                for line in lines:
                    if 'description:' in line and ("'" in line or '"' in line):
                        quote_char = "'" if "'" in line else '"'
                        start = line.find(quote_char) + 1
                        end = line.find(quote_char, start)
                        if end > start:
                            node_info['description'] = line[start:end]
                            break
            
            if node_info:
                # å„²å­˜æå–çš„è³‡è¨Š
                safe_name = node_file.stem.replace('.node', '')
                output_file = output_dir / f"{safe_name}_schema.json"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(node_info, f, indent=2, ensure_ascii=False)
                
                extracted_schemas.append(node_info)
                print(f"    âœ… {node_info.get('displayName', safe_name)}")
            
        except Exception as e:
            print(f"    âŒ æå– {node_file.name} å¤±æ•—: {e}")
    
    print(f"\nğŸ“Š æå–çµæœ: {len(extracted_schemas)} å€‹æ–° schema")
    
    return extracted_schemas

def main():
    """ä¸»å‡½æ•¸"""
    
    print("=" * 80)
    print("ğŸš€ n8n ç¤¾ç¾¤ç¯€é»å¤§è¦æ¨¡ä¸‹è¼‰å™¨")
    print("=" * 80)
    
    # 1. ä¸‹è¼‰å„ªé¸å¥—ä»¶
    downloaded, failed = download_priority_packages()
    
    print(f"\nğŸ“Š ä¸‹è¼‰çµæœ:")
    print(f"  æˆåŠŸ: {len(downloaded)} å€‹")
    print(f"  å¤±æ•—: {len(failed)} å€‹")
    
    if failed:
        print(f"\nâŒ å¤±æ•—çš„å¥—ä»¶:")
        for pkg in failed:
            print(f"    - {pkg}")
    
    # 2. åˆ†æå¥—ä»¶
    if downloaded:
        analysis = analyze_new_packages(downloaded)
        
        # 3. æå– schema
        extracted_schemas = extract_schemas_from_new_packages()
        
        # 4. è¨ˆç®—ç¸½æ•¸
        print(f"\n" + "=" * 80)
        print("ğŸ¯ æœ€çµ‚çµ±è¨ˆ")
        print("=" * 80)
        
        # ä¹‹å‰çš„æ•¸æ“š
        original_schemas = 792  # åŸå§‹ API
        community_schemas = 30   # ä¹‹å‰çš„ç¤¾ç¾¤å¥—ä»¶
        official_nodes = 458     # å®˜æ–¹ nodes-base
        new_schemas = len(extracted_schemas)  # æ–°çš„ç¤¾ç¾¤å¥—ä»¶
        
        total_schemas = original_schemas + community_schemas + new_schemas
        
        print(f"ğŸ“Š Schema æ•¸é‡çµ±è¨ˆ:")
        print(f"  åŸå§‹ API Schema: {original_schemas}")
        print(f"  ä¹‹å‰ç¤¾ç¾¤ Schema: {community_schemas}")
        print(f"  æ–°å¢ç¤¾ç¾¤ Schema: {new_schemas}")
        print(f"  å®˜æ–¹ nodes-base: {official_nodes} (å¯èƒ½é‡è¤‡)")
        print(f"  ç¸½è¨ˆ: {total_schemas}")
        
        print(f"\nğŸ‰ é€²åº¦:")
        print(f"  ç›®å‰ Schema æ•¸: {total_schemas}")
        print(f"  å®˜æ–¹ç›®æ¨™æ•¸: 1157")
        print(f"  é”æˆç‡: {(total_schemas/1157*100):.1f}%")
        print(f"  é‚„éœ€è¦: {max(0, 1157-total_schemas)} å€‹")
        
        # 5. å„²å­˜çµæœ
        results = {
            'downloaded_packages': downloaded,
            'failed_packages': failed,
            'package_analysis': analysis,
            'extracted_schemas': extracted_schemas,
            'statistics': {
                'original_schemas': original_schemas,
                'previous_community': community_schemas,
                'new_community': new_schemas,
                'official_nodes': official_nodes,
                'total': total_schemas,
                'target': 1157,
                'completion_rate': total_schemas/1157*100
            },
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('batch_download_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²å„²å­˜è‡³: batch_download_results.json")

if __name__ == "__main__":
    main()