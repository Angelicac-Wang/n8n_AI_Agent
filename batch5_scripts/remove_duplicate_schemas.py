#!/usr/bin/env python3
import os
import re

def normalize_filename(filename):
    """å°‡æª”æ¡ˆåæ¨™æº–åŒ–ç‚ºå°å¯«ï¼Œç§»é™¤å‰¯æª”å"""
    # ç§»é™¤å‰¯æª”å
    name = filename.replace('.json', '')
    # ç§»é™¤å¯èƒ½çš„å¾Œç¶´
    name = re.sub(r'_schema$', '', name)
    # è½‰ç‚ºå°å¯«
    return name.lower()

def analyze_duplicates():
    """åˆ†æå…©å€‹è³‡æ–™å¤¾ä¸­çš„é‡è¤‡æª”æ¡ˆ"""
    base_path = "/Users/angelicawang/Documents/n8n/n8n_json_schema"
    
    # ç¬¬ä¸€æ¬¡ fetch æª”æ¡ˆ (node_schemas)
    first_path = os.path.join(base_path, "node_schemas")
    first_files = {}
    if os.path.exists(first_path):
        for filename in os.listdir(first_path):
            if filename.endswith('.json') and not filename.startswith('.'):
                normalized = normalize_filename(filename)
                first_files[normalized] = filename
    
    # ç¬¬ä¸‰æ¬¡ fetch æª”æ¡ˆ (nodes_base_schemas)  
    third_path = os.path.join(base_path, "nodes_base_schemas")
    third_files = {}
    if os.path.exists(third_path):
        for filename in os.listdir(third_path):
            if filename.endswith('.json') and not filename.startswith('.'):
                normalized = normalize_filename(filename)
                third_files[normalized] = filename
    
    # æ‰¾å‡ºé‡è¤‡çš„æª”æ¡ˆ
    duplicates = []
    unique_to_third = []
    
    for normalized_name in third_files:
        if normalized_name in first_files:
            duplicates.append({
                'normalized': normalized_name,
                'first_file': first_files[normalized_name],
                'third_file': third_files[normalized_name],
                'third_path': os.path.join(third_path, third_files[normalized_name])
            })
        else:
            unique_to_third.append({
                'normalized': normalized_name,
                'third_file': third_files[normalized_name]
            })
    
    print(f"ğŸ“Š é‡è¤‡åˆ†æçµæœ:")
    print(f"ç¬¬ä¸€æ¬¡ fetch æª”æ¡ˆæ•¸: {len(first_files)}")
    print(f"ç¬¬ä¸‰æ¬¡ fetch æª”æ¡ˆæ•¸: {len(third_files)}")
    print(f"é‡è¤‡æª”æ¡ˆæ•¸: {len(duplicates)}")
    print(f"ç¬¬ä¸‰æ¬¡ç¨æœ‰æª”æ¡ˆæ•¸: {len(unique_to_third)}")
    
    print(f"\\nğŸ”„ é‡è¤‡æª”æ¡ˆç¯„ä¾‹ (å‰10å€‹):")
    for i, dup in enumerate(duplicates[:10]):
        print(f"  {i+1}. {dup['normalized']}")
        print(f"     ç¬¬ä¸€æ¬¡: {dup['first_file']}")
        print(f"     ç¬¬ä¸‰æ¬¡: {dup['third_file']}")
    
    print(f"\\nğŸ†• ç¬¬ä¸‰æ¬¡ç¨æœ‰æª”æ¡ˆ (é‡è¦çš„æ–°å¢):")
    for unique in unique_to_third:
        print(f"  - {unique['third_file']}")
    
    return duplicates, unique_to_third

def remove_duplicates(duplicates):
    """ç§»é™¤é‡è¤‡æª”æ¡ˆ"""
    print(f"\\nğŸ—‘ï¸ é–‹å§‹ç§»é™¤é‡è¤‡æª”æ¡ˆ...")
    
    removed_count = 0
    for dup in duplicates:
        try:
            os.remove(dup['third_path'])
            print(f"âœ… å·²åˆªé™¤: {dup['third_file']}")
            removed_count += 1
        except Exception as e:
            print(f"âŒ åˆªé™¤å¤±æ•—: {dup['third_file']} - {e}")
    
    print(f"\\nğŸ“Š åˆªé™¤ç¸½çµ:")
    print(f"æˆåŠŸåˆªé™¤: {removed_count} å€‹æª”æ¡ˆ")
    print(f"åˆªé™¤å¤±æ•—: {len(duplicates) - removed_count} å€‹æª”æ¡ˆ")
    
    return removed_count

if __name__ == "__main__":
    duplicates, unique_to_third = analyze_duplicates()
    
    if duplicates:
        print(f"\\nâ“ ç¢ºèªè¦åˆªé™¤ {len(duplicates)} å€‹é‡è¤‡æª”æ¡ˆå—ï¼Ÿ")
        print("é€™äº›æª”æ¡ˆåœ¨ç¬¬ä¸€æ¬¡ fetch ä¸­å·²ç¶“å­˜åœ¨ï¼ˆåªæ˜¯å¤§å°å¯«ä¸åŒï¼‰")
        
        # è‡ªå‹•åŸ·è¡Œåˆªé™¤
        removed = remove_duplicates(duplicates)
        
        print(f"\\nğŸ¯ æ¸…ç†å¾Œçš„ nodes_base_schemas è³‡æ–™å¤¾:")
        print(f"ä¿ç•™æª”æ¡ˆæ•¸: {len(unique_to_third)}")
        print("é€™äº›æ˜¯çœŸæ­£æ–°å¢çš„ã€ç¬¬ä¸€æ¬¡ fetch æ²’æœ‰çš„æª”æ¡ˆ")
    else:
        print("\\nâœ… æ²’æœ‰ç™¼ç¾é‡è¤‡æª”æ¡ˆ")